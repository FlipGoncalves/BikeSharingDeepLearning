{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 15:51:03.698400: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-31 15:51:03.730281: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-31 15:51:03.731784: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-31 15:51:04.505756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../StationDateDatasets/datasetStationDate.csv\").drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"StationEnd\", \"WorkingDay\", \"Hour\", \"Count\", \"Count1\", \"Count1week\", \"Count2week\", \"Count3week\", \"Temp\", \"ATemp\", \"WeatherSituation\"]\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 1\n",
    "verbose = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 16:50:34.427661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-31 16:50:34.428334: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 16:50:34.862915: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-31 16:50:34.864847: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-31 16:50:34.865767: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-31 16:50:35.233428: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-31 16:50:35.235630: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-31 16:50:35.236976: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-31 16:50:35.733180: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-31 16:50:35.735053: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-31 16:50:35.736876: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183062/183062 - 164s - loss: 4.1198 - accuracy: 0.3202 - 164s/epoch - 897us/step\n",
      "Epoch 2/30\n",
      "183062/183062 - 159s - loss: 3.9421 - accuracy: 0.3205 - 159s/epoch - 870us/step\n",
      "Epoch 3/30\n",
      "183062/183062 - 160s - loss: 3.9173 - accuracy: 0.3208 - 160s/epoch - 873us/step\n",
      "Epoch 4/30\n",
      "183062/183062 - 160s - loss: 3.9017 - accuracy: 0.3207 - 160s/epoch - 873us/step\n",
      "Epoch 5/30\n",
      "183062/183062 - 160s - loss: 3.8968 - accuracy: 0.3207 - 160s/epoch - 873us/step\n",
      "Epoch 6/30\n",
      "183062/183062 - 160s - loss: 3.8893 - accuracy: 0.3207 - 160s/epoch - 872us/step\n",
      "Epoch 7/30\n",
      "183062/183062 - 161s - loss: 3.8885 - accuracy: 0.3208 - 161s/epoch - 879us/step\n",
      "Epoch 8/30\n",
      "183062/183062 - 157s - loss: 3.8788 - accuracy: 0.3207 - 157s/epoch - 859us/step\n",
      "Epoch 9/30\n",
      "183062/183062 - 159s - loss: 3.8728 - accuracy: 0.3207 - 159s/epoch - 869us/step\n",
      "Epoch 10/30\n",
      "183062/183062 - 159s - loss: 3.8601 - accuracy: 0.3204 - 159s/epoch - 869us/step\n",
      "Epoch 11/30\n",
      "183062/183062 - 159s - loss: 3.8247 - accuracy: 0.3191 - 159s/epoch - 871us/step\n",
      "Epoch 12/30\n",
      "183062/183062 - 159s - loss: 3.8082 - accuracy: 0.3186 - 159s/epoch - 871us/step\n",
      "Epoch 13/30\n",
      "183062/183062 - 160s - loss: 3.7897 - accuracy: 0.3184 - 160s/epoch - 871us/step\n",
      "Epoch 14/30\n",
      "183062/183062 - 160s - loss: 3.7794 - accuracy: 0.3199 - 160s/epoch - 871us/step\n",
      "Epoch 15/30\n",
      "183062/183062 - 159s - loss: 3.7747 - accuracy: 0.3198 - 159s/epoch - 870us/step\n",
      "Epoch 16/30\n",
      "183062/183062 - 159s - loss: 3.7688 - accuracy: 0.3199 - 159s/epoch - 871us/step\n",
      "Epoch 17/30\n",
      "183062/183062 - 159s - loss: 3.7679 - accuracy: 0.3200 - 159s/epoch - 868us/step\n",
      "Epoch 18/30\n",
      "183062/183062 - 159s - loss: 3.7633 - accuracy: 0.3200 - 159s/epoch - 870us/step\n",
      "Epoch 19/30\n",
      "183062/183062 - 159s - loss: 3.7574 - accuracy: 0.3201 - 159s/epoch - 868us/step\n",
      "Epoch 20/30\n",
      "183062/183062 - 159s - loss: 3.7599 - accuracy: 0.3200 - 159s/epoch - 871us/step\n",
      "Epoch 21/30\n",
      "183062/183062 - 159s - loss: 3.7574 - accuracy: 0.3200 - 159s/epoch - 870us/step\n",
      "Epoch 22/30\n",
      "183062/183062 - 160s - loss: 3.7487 - accuracy: 0.3200 - 160s/epoch - 873us/step\n",
      "Epoch 23/30\n",
      "183062/183062 - 160s - loss: 3.7487 - accuracy: 0.3198 - 160s/epoch - 872us/step\n",
      "Epoch 24/30\n",
      "183062/183062 - 160s - loss: 3.7545 - accuracy: 0.3198 - 160s/epoch - 871us/step\n",
      "Epoch 25/30\n",
      "183062/183062 - 159s - loss: 3.7492 - accuracy: 0.3198 - 159s/epoch - 870us/step\n",
      "Epoch 26/30\n",
      "183062/183062 - 159s - loss: 3.7508 - accuracy: 0.3196 - 159s/epoch - 869us/step\n",
      "Epoch 27/30\n",
      "183062/183062 - 159s - loss: 3.7481 - accuracy: 0.3194 - 159s/epoch - 870us/step\n",
      "Epoch 28/30\n",
      "183062/183062 - 159s - loss: 3.7489 - accuracy: 0.3197 - 159s/epoch - 868us/step\n",
      "Epoch 29/30\n",
      "183062/183062 - 159s - loss: 3.7484 - accuracy: 0.3197 - 159s/epoch - 870us/step\n",
      "Epoch 30/30\n",
      "183062/183062 - 160s - loss: 3.7460 - accuracy: 0.3198 - 160s/epoch - 873us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 18:10:21.182147: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-31 18:10:21.183550: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-31 18:10:21.184837: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "183062/183062 - 162s - loss: 3.5883 - accuracy: 0.3335 - 162s/epoch - 882us/step\n",
      "Epoch 2/30\n",
      "183062/183062 - 161s - loss: 3.5934 - accuracy: 0.3338 - 161s/epoch - 882us/step\n",
      "Epoch 3/30\n",
      "183062/183062 - 162s - loss: 3.5892 - accuracy: 0.3337 - 162s/epoch - 883us/step\n",
      "Epoch 4/30\n",
      "183062/183062 - 161s - loss: 3.5856 - accuracy: 0.3336 - 161s/epoch - 880us/step\n",
      "Epoch 5/30\n",
      "183062/183062 - 161s - loss: 3.5849 - accuracy: 0.3338 - 161s/epoch - 880us/step\n",
      "Epoch 6/30\n",
      "183062/183062 - 161s - loss: 3.5832 - accuracy: 0.3337 - 161s/epoch - 879us/step\n",
      "Epoch 7/30\n",
      "183062/183062 - 161s - loss: 3.5860 - accuracy: 0.3338 - 161s/epoch - 879us/step\n",
      "Epoch 8/30\n",
      "183062/183062 - 160s - loss: 3.5855 - accuracy: 0.3338 - 160s/epoch - 876us/step\n",
      "Epoch 9/30\n",
      "183062/183062 - 161s - loss: 3.5895 - accuracy: 0.3337 - 161s/epoch - 878us/step\n",
      "Epoch 10/30\n",
      "183062/183062 - 161s - loss: 3.5805 - accuracy: 0.3338 - 161s/epoch - 880us/step\n",
      "Epoch 11/30\n",
      "183062/183062 - 161s - loss: 3.5839 - accuracy: 0.3338 - 161s/epoch - 879us/step\n",
      "Epoch 12/30\n",
      "183062/183062 - 161s - loss: 3.5871 - accuracy: 0.3338 - 161s/epoch - 878us/step\n",
      "Epoch 13/30\n",
      "183062/183062 - 161s - loss: 3.5850 - accuracy: 0.3339 - 161s/epoch - 879us/step\n",
      "Epoch 14/30\n",
      "183062/183062 - 161s - loss: 3.5871 - accuracy: 0.3337 - 161s/epoch - 879us/step\n",
      "Epoch 15/30\n",
      "183062/183062 - 161s - loss: 3.5857 - accuracy: 0.3339 - 161s/epoch - 881us/step\n",
      "Epoch 16/30\n",
      "183062/183062 - 161s - loss: 3.5816 - accuracy: 0.3337 - 161s/epoch - 880us/step\n",
      "Epoch 17/30\n",
      "183062/183062 - 161s - loss: 3.5863 - accuracy: 0.3338 - 161s/epoch - 877us/step\n",
      "Epoch 18/30\n",
      "183062/183062 - 160s - loss: 3.5795 - accuracy: 0.3340 - 160s/epoch - 876us/step\n",
      "Epoch 19/30\n",
      "183062/183062 - 161s - loss: 3.5846 - accuracy: 0.3340 - 161s/epoch - 879us/step\n",
      "Epoch 20/30\n",
      "183062/183062 - 161s - loss: 3.5840 - accuracy: 0.3340 - 161s/epoch - 877us/step\n",
      "Epoch 21/30\n",
      "183062/183062 - 160s - loss: 3.5864 - accuracy: 0.3340 - 160s/epoch - 876us/step\n",
      "Epoch 22/30\n",
      "183062/183062 - 161s - loss: 3.5878 - accuracy: 0.3339 - 161s/epoch - 878us/step\n",
      "Epoch 23/30\n",
      "183062/183062 - 161s - loss: 3.5824 - accuracy: 0.3338 - 161s/epoch - 878us/step\n",
      "Epoch 24/30\n",
      "183062/183062 - 160s - loss: 3.5820 - accuracy: 0.3339 - 160s/epoch - 876us/step\n",
      "Epoch 25/30\n",
      "183062/183062 - 160s - loss: 3.5823 - accuracy: 0.3339 - 160s/epoch - 876us/step\n",
      "Epoch 26/30\n",
      "183062/183062 - 160s - loss: 3.5788 - accuracy: 0.3339 - 160s/epoch - 875us/step\n",
      "Epoch 27/30\n",
      "183062/183062 - 161s - loss: 3.5766 - accuracy: 0.3341 - 161s/epoch - 877us/step\n",
      "Epoch 28/30\n",
      "183062/183062 - 161s - loss: 3.5782 - accuracy: 0.3340 - 161s/epoch - 878us/step\n",
      "Epoch 29/30\n",
      "183062/183062 - 161s - loss: 3.5784 - accuracy: 0.3339 - 161s/epoch - 879us/step\n",
      "Epoch 30/30\n",
      "183062/183062 - 160s - loss: 3.5740 - accuracy: 0.3340 - 160s/epoch - 875us/step\n",
      "Epoch 1/30\n",
      "183062/183062 - 160s - loss: 3.6012 - accuracy: 0.3322 - 160s/epoch - 876us/step\n",
      "Epoch 2/30\n",
      "183062/183062 - 160s - loss: 3.5991 - accuracy: 0.3322 - 160s/epoch - 876us/step\n",
      "Epoch 3/30\n",
      "183062/183062 - 160s - loss: 3.6022 - accuracy: 0.3322 - 160s/epoch - 877us/step\n",
      "Epoch 4/30\n",
      "183062/183062 - 161s - loss: 3.6031 - accuracy: 0.3322 - 161s/epoch - 878us/step\n",
      "Epoch 5/30\n",
      "183062/183062 - 161s - loss: 3.6074 - accuracy: 0.3322 - 161s/epoch - 878us/step\n",
      "Epoch 6/30\n",
      "183062/183062 - 161s - loss: 3.6006 - accuracy: 0.3322 - 161s/epoch - 878us/step\n",
      "Epoch 7/30\n",
      "183062/183062 - 160s - loss: 3.6027 - accuracy: 0.3322 - 160s/epoch - 876us/step\n",
      "Epoch 8/30\n",
      "183062/183062 - 160s - loss: 3.5967 - accuracy: 0.3323 - 160s/epoch - 877us/step\n",
      "Epoch 9/30\n",
      "183062/183062 - 160s - loss: 3.5981 - accuracy: 0.3323 - 160s/epoch - 877us/step\n",
      "Epoch 10/30\n",
      "183062/183062 - 160s - loss: 3.5973 - accuracy: 0.3323 - 160s/epoch - 876us/step\n",
      "Epoch 11/30\n",
      "183062/183062 - 160s - loss: 3.6019 - accuracy: 0.3323 - 160s/epoch - 876us/step\n",
      "Epoch 12/30\n",
      "183062/183062 - 160s - loss: 3.6029 - accuracy: 0.3323 - 160s/epoch - 876us/step\n",
      "Epoch 13/30\n",
      "183062/183062 - 160s - loss: 3.5991 - accuracy: 0.3323 - 160s/epoch - 876us/step\n",
      "Epoch 14/30\n",
      "183062/183062 - 161s - loss: 3.5994 - accuracy: 0.3323 - 161s/epoch - 877us/step\n",
      "Epoch 15/30\n",
      "183062/183062 - 160s - loss: 3.6000 - accuracy: 0.3323 - 160s/epoch - 877us/step\n",
      "Epoch 16/30\n",
      "183062/183062 - 161s - loss: 3.5971 - accuracy: 0.3322 - 161s/epoch - 877us/step\n",
      "Epoch 17/30\n",
      "183062/183062 - 160s - loss: 3.6000 - accuracy: 0.3324 - 160s/epoch - 877us/step\n",
      "Epoch 18/30\n",
      "183062/183062 - 161s - loss: 3.6000 - accuracy: 0.3323 - 161s/epoch - 878us/step\n",
      "Epoch 19/30\n",
      "183062/183062 - 160s - loss: 3.5958 - accuracy: 0.3322 - 160s/epoch - 876us/step\n",
      "Epoch 20/30\n",
      "183062/183062 - 161s - loss: 3.5991 - accuracy: 0.3323 - 161s/epoch - 877us/step\n",
      "Epoch 21/30\n",
      "183062/183062 - 160s - loss: 3.6046 - accuracy: 0.3323 - 160s/epoch - 877us/step\n",
      "Epoch 22/30\n",
      "183062/183062 - 160s - loss: 3.5965 - accuracy: 0.3323 - 160s/epoch - 877us/step\n",
      "Epoch 23/30\n",
      "183062/183062 - 160s - loss: 3.5984 - accuracy: 0.3323 - 160s/epoch - 875us/step\n",
      "Epoch 24/30\n",
      "183062/183062 - 160s - loss: 3.5995 - accuracy: 0.3323 - 160s/epoch - 874us/step\n",
      "Epoch 25/30\n",
      "183062/183062 - 160s - loss: 3.5988 - accuracy: 0.3323 - 160s/epoch - 876us/step\n",
      "Epoch 26/30\n",
      "183062/183062 - 161s - loss: 3.6018 - accuracy: 0.3322 - 161s/epoch - 877us/step\n",
      "Epoch 27/30\n",
      "183062/183062 - 160s - loss: 3.6022 - accuracy: 0.3323 - 160s/epoch - 876us/step\n",
      "Epoch 28/30\n",
      "183062/183062 - 160s - loss: 3.6036 - accuracy: 0.3323 - 160s/epoch - 876us/step\n",
      "Epoch 29/30\n",
      "183062/183062 - 160s - loss: 3.5991 - accuracy: 0.3324 - 160s/epoch - 876us/step\n",
      "Epoch 30/30\n",
      "183062/183062 - 161s - loss: 3.6043 - accuracy: 0.3324 - 161s/epoch - 877us/step\n",
      "Epoch 1/30\n",
      "183063/183063 - 161s - loss: 3.6525 - accuracy: 0.3273 - 161s/epoch - 877us/step\n",
      "Epoch 2/30\n",
      "183063/183063 - 161s - loss: 3.6529 - accuracy: 0.3272 - 161s/epoch - 878us/step\n",
      "Epoch 3/30\n",
      "183063/183063 - 161s - loss: 3.6550 - accuracy: 0.3273 - 161s/epoch - 879us/step\n",
      "Epoch 4/30\n",
      "183063/183063 - 161s - loss: 3.6582 - accuracy: 0.3273 - 161s/epoch - 878us/step\n",
      "Epoch 5/30\n",
      "183063/183063 - 161s - loss: 3.6510 - accuracy: 0.3272 - 161s/epoch - 877us/step\n",
      "Epoch 6/30\n",
      "183063/183063 - 161s - loss: 3.6532 - accuracy: 0.3273 - 161s/epoch - 878us/step\n",
      "Epoch 7/30\n",
      "183063/183063 - 161s - loss: 3.6532 - accuracy: 0.3272 - 161s/epoch - 877us/step\n",
      "Epoch 8/30\n",
      "183063/183063 - 161s - loss: 3.6559 - accuracy: 0.3273 - 161s/epoch - 878us/step\n",
      "Epoch 9/30\n",
      "183063/183063 - 160s - loss: 3.6550 - accuracy: 0.3273 - 160s/epoch - 877us/step\n",
      "Epoch 10/30\n",
      "183063/183063 - 160s - loss: 3.6526 - accuracy: 0.3274 - 160s/epoch - 876us/step\n",
      "Epoch 11/30\n",
      "183063/183063 - 160s - loss: 3.6515 - accuracy: 0.3273 - 160s/epoch - 876us/step\n",
      "Epoch 12/30\n",
      "183063/183063 - 161s - loss: 3.6478 - accuracy: 0.3274 - 161s/epoch - 879us/step\n",
      "Epoch 13/30\n",
      "183063/183063 - 161s - loss: 3.6511 - accuracy: 0.3274 - 161s/epoch - 878us/step\n",
      "Epoch 14/30\n",
      "183063/183063 - 161s - loss: 3.6557 - accuracy: 0.3273 - 161s/epoch - 877us/step\n",
      "Epoch 15/30\n",
      "183063/183063 - 160s - loss: 3.6490 - accuracy: 0.3272 - 160s/epoch - 876us/step\n",
      "Epoch 16/30\n",
      "183063/183063 - 160s - loss: 3.6482 - accuracy: 0.3273 - 160s/epoch - 876us/step\n",
      "Epoch 17/30\n",
      "183063/183063 - 160s - loss: 3.6531 - accuracy: 0.3274 - 160s/epoch - 876us/step\n",
      "Epoch 18/30\n",
      "183063/183063 - 161s - loss: 3.6511 - accuracy: 0.3272 - 161s/epoch - 877us/step\n",
      "Epoch 19/30\n",
      "183063/183063 - 161s - loss: 3.6511 - accuracy: 0.3273 - 161s/epoch - 877us/step\n",
      "Epoch 20/30\n",
      "183063/183063 - 161s - loss: 3.6510 - accuracy: 0.3273 - 161s/epoch - 877us/step\n",
      "Epoch 21/30\n",
      "183063/183063 - 161s - loss: 3.6536 - accuracy: 0.3274 - 161s/epoch - 879us/step\n",
      "Epoch 22/30\n",
      "183063/183063 - 161s - loss: 3.6483 - accuracy: 0.3273 - 161s/epoch - 881us/step\n",
      "Epoch 23/30\n",
      "183063/183063 - 161s - loss: 3.6503 - accuracy: 0.3274 - 161s/epoch - 881us/step\n",
      "Epoch 24/30\n",
      "183063/183063 - 161s - loss: 3.6479 - accuracy: 0.3274 - 161s/epoch - 880us/step\n",
      "Epoch 25/30\n",
      "183063/183063 - 161s - loss: 3.6499 - accuracy: 0.3274 - 161s/epoch - 879us/step\n",
      "Epoch 26/30\n",
      "183063/183063 - 161s - loss: 3.6545 - accuracy: 0.3273 - 161s/epoch - 877us/step\n",
      "Epoch 27/30\n",
      "183063/183063 - 160s - loss: 3.6525 - accuracy: 0.3274 - 160s/epoch - 876us/step\n",
      "Epoch 28/30\n",
      "183063/183063 - 160s - loss: 3.6515 - accuracy: 0.3274 - 160s/epoch - 874us/step\n",
      "Epoch 29/30\n",
      "183063/183063 - 160s - loss: 3.6456 - accuracy: 0.3274 - 160s/epoch - 874us/step\n",
      "Epoch 30/30\n",
      "183063/183063 - 160s - loss: 3.6511 - accuracy: 0.3274 - 160s/epoch - 876us/step\n",
      "Epoch 1/30\n",
      "183063/183063 - 161s - loss: 3.7211 - accuracy: 0.3238 - 161s/epoch - 877us/step\n",
      "Epoch 2/30\n",
      "183063/183063 - 161s - loss: 3.7228 - accuracy: 0.3239 - 161s/epoch - 877us/step\n",
      "Epoch 3/30\n",
      "183063/183063 - 161s - loss: 3.7180 - accuracy: 0.3239 - 161s/epoch - 877us/step\n",
      "Epoch 4/30\n",
      "183063/183063 - 161s - loss: 3.7228 - accuracy: 0.3239 - 161s/epoch - 877us/step\n",
      "Epoch 5/30\n",
      "183063/183063 - 160s - loss: 3.7233 - accuracy: 0.3238 - 160s/epoch - 876us/step\n",
      "Epoch 6/30\n",
      "183063/183063 - 161s - loss: 3.7198 - accuracy: 0.3238 - 161s/epoch - 878us/step\n",
      "Epoch 7/30\n",
      "183063/183063 - 161s - loss: 3.7139 - accuracy: 0.3238 - 161s/epoch - 877us/step\n",
      "Epoch 8/30\n",
      "183063/183063 - 160s - loss: 3.7168 - accuracy: 0.3238 - 160s/epoch - 876us/step\n",
      "Epoch 9/30\n",
      "183063/183063 - 161s - loss: 3.7238 - accuracy: 0.3237 - 161s/epoch - 878us/step\n",
      "Epoch 10/30\n",
      "183063/183063 - 160s - loss: 3.7163 - accuracy: 0.3238 - 160s/epoch - 876us/step\n",
      "Epoch 11/30\n",
      "183063/183063 - 160s - loss: 3.7198 - accuracy: 0.3238 - 160s/epoch - 875us/step\n",
      "Epoch 12/30\n",
      "183063/183063 - 160s - loss: 3.7201 - accuracy: 0.3238 - 160s/epoch - 876us/step\n",
      "Epoch 13/30\n",
      "183063/183063 - 161s - loss: 3.7242 - accuracy: 0.3238 - 161s/epoch - 878us/step\n",
      "Epoch 14/30\n",
      "183063/183063 - 160s - loss: 3.7242 - accuracy: 0.3239 - 160s/epoch - 874us/step\n",
      "Epoch 15/30\n",
      "183063/183063 - 161s - loss: 3.7218 - accuracy: 0.3239 - 161s/epoch - 878us/step\n",
      "Epoch 16/30\n",
      "183063/183063 - 160s - loss: 3.7216 - accuracy: 0.3238 - 160s/epoch - 876us/step\n",
      "Epoch 17/30\n",
      "183063/183063 - 160s - loss: 3.7193 - accuracy: 0.3238 - 160s/epoch - 874us/step\n",
      "Epoch 18/30\n",
      "183063/183063 - 160s - loss: 3.7180 - accuracy: 0.3238 - 160s/epoch - 874us/step\n",
      "Epoch 19/30\n",
      "183063/183063 - 160s - loss: 3.7234 - accuracy: 0.3238 - 160s/epoch - 876us/step\n",
      "Epoch 20/30\n",
      "183063/183063 - 160s - loss: 3.7161 - accuracy: 0.3237 - 160s/epoch - 876us/step\n",
      "Epoch 21/30\n",
      "183063/183063 - 160s - loss: 3.7191 - accuracy: 0.3238 - 160s/epoch - 876us/step\n",
      "Epoch 22/30\n",
      "183063/183063 - 160s - loss: 3.7149 - accuracy: 0.3239 - 160s/epoch - 876us/step\n",
      "Epoch 23/30\n",
      "183063/183063 - 160s - loss: 3.7156 - accuracy: 0.3237 - 160s/epoch - 875us/step\n",
      "Epoch 24/30\n",
      "183063/183063 - 160s - loss: 3.7227 - accuracy: 0.3238 - 160s/epoch - 875us/step\n",
      "Epoch 25/30\n",
      "183063/183063 - 160s - loss: 3.7235 - accuracy: 0.3238 - 160s/epoch - 876us/step\n",
      "Epoch 26/30\n",
      "183063/183063 - 161s - loss: 3.7221 - accuracy: 0.3238 - 161s/epoch - 877us/step\n",
      "Epoch 27/30\n",
      "183063/183063 - 161s - loss: 3.7216 - accuracy: 0.3239 - 161s/epoch - 877us/step\n",
      "Epoch 28/30\n",
      "183063/183063 - 161s - loss: 3.7184 - accuracy: 0.3238 - 161s/epoch - 877us/step\n",
      "Epoch 29/30\n",
      "183063/183063 - 160s - loss: 3.7194 - accuracy: 0.3238 - 160s/epoch - 877us/step\n",
      "Epoch 30/30\n",
      "183063/183063 - 160s - loss: 3.7177 - accuracy: 0.3239 - 160s/epoch - 876us/step\n",
      "Epoch 1/30\n",
      "183063/183063 - 161s - loss: 3.5444 - accuracy: 0.3372 - 161s/epoch - 877us/step\n",
      "Epoch 2/30\n",
      "183063/183063 - 161s - loss: 3.5413 - accuracy: 0.3372 - 161s/epoch - 878us/step\n",
      "Epoch 3/30\n",
      "183063/183063 - 161s - loss: 3.5456 - accuracy: 0.3373 - 161s/epoch - 877us/step\n",
      "Epoch 4/30\n",
      "183063/183063 - 161s - loss: 3.5479 - accuracy: 0.3373 - 161s/epoch - 878us/step\n",
      "Epoch 5/30\n",
      "183063/183063 - 161s - loss: 3.5399 - accuracy: 0.3373 - 161s/epoch - 877us/step\n",
      "Epoch 6/30\n",
      "183063/183063 - 161s - loss: 3.5436 - accuracy: 0.3373 - 161s/epoch - 878us/step\n",
      "Epoch 7/30\n",
      "183063/183063 - 160s - loss: 3.5436 - accuracy: 0.3372 - 160s/epoch - 876us/step\n",
      "Epoch 8/30\n",
      "183063/183063 - 160s - loss: 3.5428 - accuracy: 0.3373 - 160s/epoch - 876us/step\n",
      "Epoch 9/30\n",
      "183063/183063 - 160s - loss: 3.5436 - accuracy: 0.3373 - 160s/epoch - 876us/step\n",
      "Epoch 10/30\n",
      "183063/183063 - 160s - loss: 3.5436 - accuracy: 0.3372 - 160s/epoch - 876us/step\n",
      "Epoch 11/30\n",
      "183063/183063 - 160s - loss: 3.5439 - accuracy: 0.3373 - 160s/epoch - 876us/step\n",
      "Epoch 12/30\n",
      "183063/183063 - 161s - loss: 3.5426 - accuracy: 0.3373 - 161s/epoch - 877us/step\n",
      "Epoch 13/30\n",
      "183063/183063 - 161s - loss: 3.5367 - accuracy: 0.3373 - 161s/epoch - 877us/step\n",
      "Epoch 14/30\n",
      "183063/183063 - 161s - loss: 3.5429 - accuracy: 0.3373 - 161s/epoch - 877us/step\n",
      "Epoch 15/30\n",
      "183063/183063 - 160s - loss: 3.5427 - accuracy: 0.3372 - 160s/epoch - 876us/step\n",
      "Epoch 16/30\n",
      "183063/183063 - 160s - loss: 3.5415 - accuracy: 0.3373 - 160s/epoch - 875us/step\n",
      "Epoch 17/30\n",
      "183063/183063 - 160s - loss: 3.5420 - accuracy: 0.3372 - 160s/epoch - 875us/step\n",
      "Epoch 18/30\n",
      "183063/183063 - 161s - loss: 3.5416 - accuracy: 0.3373 - 161s/epoch - 878us/step\n",
      "Epoch 19/30\n",
      "183063/183063 - 161s - loss: 3.5369 - accuracy: 0.3373 - 161s/epoch - 879us/step\n",
      "Epoch 20/30\n",
      "183063/183063 - 161s - loss: 3.5468 - accuracy: 0.3374 - 161s/epoch - 877us/step\n",
      "Epoch 21/30\n",
      "183063/183063 - 160s - loss: 3.5395 - accuracy: 0.3372 - 160s/epoch - 876us/step\n",
      "Epoch 22/30\n",
      "183063/183063 - 161s - loss: 3.5418 - accuracy: 0.3373 - 161s/epoch - 877us/step\n",
      "Epoch 23/30\n",
      "183063/183063 - 160s - loss: 3.5392 - accuracy: 0.3374 - 160s/epoch - 875us/step\n",
      "Epoch 24/30\n",
      "183063/183063 - 160s - loss: 3.5386 - accuracy: 0.3372 - 160s/epoch - 876us/step\n",
      "Epoch 25/30\n",
      "183063/183063 - 161s - loss: 3.5424 - accuracy: 0.3373 - 161s/epoch - 879us/step\n",
      "Epoch 26/30\n",
      "183063/183063 - 160s - loss: 3.5398 - accuracy: 0.3373 - 160s/epoch - 876us/step\n",
      "Epoch 27/30\n",
      "183063/183063 - 160s - loss: 3.5412 - accuracy: 0.3372 - 160s/epoch - 876us/step\n",
      "Epoch 28/30\n",
      "183063/183063 - 160s - loss: 3.5436 - accuracy: 0.3373 - 160s/epoch - 876us/step\n",
      "Epoch 29/30\n",
      "183063/183063 - 161s - loss: 3.5404 - accuracy: 0.3373 - 161s/epoch - 877us/step\n",
      "Epoch 30/30\n",
      "183063/183063 - 161s - loss: 3.5448 - accuracy: 0.3374 - 161s/epoch - 878us/step\n",
      "Epoch 1/30\n",
      "183063/183063 - 162s - loss: 3.4717 - accuracy: 0.3417 - 162s/epoch - 887us/step\n",
      "Epoch 2/30\n",
      "183063/183063 - 162s - loss: 3.4664 - accuracy: 0.3417 - 162s/epoch - 887us/step\n",
      "Epoch 3/30\n",
      "183063/183063 - 162s - loss: 3.4699 - accuracy: 0.3418 - 162s/epoch - 886us/step\n",
      "Epoch 4/30\n",
      "183063/183063 - 162s - loss: 3.4653 - accuracy: 0.3418 - 162s/epoch - 885us/step\n",
      "Epoch 5/30\n",
      "183063/183063 - 162s - loss: 3.4664 - accuracy: 0.3417 - 162s/epoch - 886us/step\n",
      "Epoch 6/30\n",
      "183063/183063 - 162s - loss: 3.4692 - accuracy: 0.3417 - 162s/epoch - 886us/step\n",
      "Epoch 7/30\n",
      "183063/183063 - 162s - loss: 3.4703 - accuracy: 0.3417 - 162s/epoch - 887us/step\n",
      "Epoch 8/30\n",
      "183063/183063 - 162s - loss: 3.4698 - accuracy: 0.3417 - 162s/epoch - 887us/step\n",
      "Epoch 9/30\n",
      "183063/183063 - 162s - loss: 3.4700 - accuracy: 0.3417 - 162s/epoch - 887us/step\n",
      "Epoch 10/30\n",
      "183063/183063 - 162s - loss: 3.4689 - accuracy: 0.3417 - 162s/epoch - 887us/step\n",
      "Epoch 11/30\n",
      "183063/183063 - 162s - loss: 3.4651 - accuracy: 0.3418 - 162s/epoch - 887us/step\n",
      "Epoch 12/30\n",
      "183063/183063 - 162s - loss: 3.4679 - accuracy: 0.3418 - 162s/epoch - 887us/step\n",
      "Epoch 13/30\n",
      "183063/183063 - 162s - loss: 3.4729 - accuracy: 0.3418 - 162s/epoch - 886us/step\n",
      "Epoch 14/30\n",
      "183063/183063 - 162s - loss: 3.4730 - accuracy: 0.3418 - 162s/epoch - 885us/step\n",
      "Epoch 15/30\n",
      "183063/183063 - 162s - loss: 3.4724 - accuracy: 0.3417 - 162s/epoch - 885us/step\n",
      "Epoch 16/30\n",
      "183063/183063 - 162s - loss: 3.4701 - accuracy: 0.3418 - 162s/epoch - 884us/step\n",
      "Epoch 17/30\n",
      "183063/183063 - 162s - loss: 3.4714 - accuracy: 0.3418 - 162s/epoch - 885us/step\n",
      "Epoch 18/30\n",
      "183063/183063 - 162s - loss: 3.4696 - accuracy: 0.3417 - 162s/epoch - 884us/step\n",
      "Epoch 19/30\n",
      "183063/183063 - 162s - loss: 3.4695 - accuracy: 0.3417 - 162s/epoch - 884us/step\n",
      "Epoch 20/30\n",
      "183063/183063 - 162s - loss: 3.4694 - accuracy: 0.3417 - 162s/epoch - 885us/step\n",
      "Epoch 21/30\n",
      "183063/183063 - 162s - loss: 3.4663 - accuracy: 0.3417 - 162s/epoch - 885us/step\n",
      "Epoch 22/30\n",
      "183063/183063 - 162s - loss: 3.4691 - accuracy: 0.3418 - 162s/epoch - 884us/step\n",
      "Epoch 23/30\n",
      "183063/183063 - 162s - loss: 3.4710 - accuracy: 0.3418 - 162s/epoch - 882us/step\n",
      "Epoch 24/30\n",
      "183063/183063 - 162s - loss: 3.4705 - accuracy: 0.3418 - 162s/epoch - 883us/step\n",
      "Epoch 25/30\n",
      "183063/183063 - 162s - loss: 3.4673 - accuracy: 0.3417 - 162s/epoch - 882us/step\n",
      "Epoch 26/30\n",
      "183063/183063 - 162s - loss: 3.4723 - accuracy: 0.3419 - 162s/epoch - 885us/step\n",
      "Epoch 27/30\n",
      "183063/183063 - 162s - loss: 3.4702 - accuracy: 0.3418 - 162s/epoch - 886us/step\n",
      "Epoch 28/30\n",
      "183063/183063 - 162s - loss: 3.4732 - accuracy: 0.3419 - 162s/epoch - 887us/step\n",
      "Epoch 29/30\n",
      "183063/183063 - 162s - loss: 3.4701 - accuracy: 0.3417 - 162s/epoch - 885us/step\n",
      "Epoch 30/30\n",
      "183063/183063 - 162s - loss: 3.4686 - accuracy: 0.3417 - 162s/epoch - 887us/step\n",
      "Epoch 1/30\n",
      "183063/183063 - 161s - loss: 3.4601 - accuracy: 0.3427 - 161s/epoch - 878us/step\n",
      "Epoch 2/30\n",
      "183063/183063 - 161s - loss: 3.4602 - accuracy: 0.3427 - 161s/epoch - 879us/step\n",
      "Epoch 3/30\n",
      "183063/183063 - 161s - loss: 3.4616 - accuracy: 0.3426 - 161s/epoch - 878us/step\n",
      "Epoch 4/30\n",
      "183063/183063 - 161s - loss: 3.4564 - accuracy: 0.3427 - 161s/epoch - 882us/step\n",
      "Epoch 5/30\n",
      "183063/183063 - 161s - loss: 3.4584 - accuracy: 0.3427 - 161s/epoch - 879us/step\n",
      "Epoch 6/30\n",
      "183063/183063 - 161s - loss: 3.4568 - accuracy: 0.3427 - 161s/epoch - 878us/step\n",
      "Epoch 7/30\n",
      "183063/183063 - 161s - loss: 3.4568 - accuracy: 0.3427 - 161s/epoch - 879us/step\n",
      "Epoch 8/30\n",
      "183063/183063 - 161s - loss: 3.4561 - accuracy: 0.3427 - 161s/epoch - 878us/step\n",
      "Epoch 9/30\n",
      "183063/183063 - 160s - loss: 3.4569 - accuracy: 0.3427 - 160s/epoch - 876us/step\n",
      "Epoch 10/30\n",
      "183063/183063 - 161s - loss: 3.4549 - accuracy: 0.3428 - 161s/epoch - 877us/step\n",
      "Epoch 11/30\n",
      "183063/183063 - 161s - loss: 3.4527 - accuracy: 0.3428 - 161s/epoch - 878us/step\n",
      "Epoch 12/30\n",
      "183063/183063 - 161s - loss: 3.4495 - accuracy: 0.3428 - 161s/epoch - 877us/step\n",
      "Epoch 13/30\n",
      "183063/183063 - 161s - loss: 3.4479 - accuracy: 0.3428 - 161s/epoch - 879us/step\n",
      "Epoch 14/30\n",
      "183063/183063 - 161s - loss: 3.4410 - accuracy: 0.3426 - 161s/epoch - 879us/step\n",
      "Epoch 15/30\n",
      "183063/183063 - 161s - loss: 3.4402 - accuracy: 0.3426 - 161s/epoch - 877us/step\n",
      "Epoch 16/30\n",
      "183063/183063 - 161s - loss: 3.4420 - accuracy: 0.3426 - 161s/epoch - 878us/step\n",
      "Epoch 17/30\n",
      "183063/183063 - 161s - loss: 3.4390 - accuracy: 0.3425 - 161s/epoch - 877us/step\n",
      "Epoch 18/30\n",
      "183063/183063 - 161s - loss: 3.4329 - accuracy: 0.3424 - 161s/epoch - 877us/step\n",
      "Epoch 19/30\n",
      "183063/183063 - 161s - loss: 3.4322 - accuracy: 0.3423 - 161s/epoch - 877us/step\n",
      "Epoch 20/30\n",
      "183063/183063 - 161s - loss: 3.4319 - accuracy: 0.3423 - 161s/epoch - 877us/step\n",
      "Epoch 21/30\n",
      "183063/183063 - 161s - loss: 3.4322 - accuracy: 0.3422 - 161s/epoch - 878us/step\n",
      "Epoch 22/30\n",
      "183063/183063 - 161s - loss: 3.4296 - accuracy: 0.3420 - 161s/epoch - 879us/step\n",
      "Epoch 23/30\n",
      "183063/183063 - 161s - loss: 3.4285 - accuracy: 0.3418 - 161s/epoch - 877us/step\n",
      "Epoch 24/30\n",
      "183063/183063 - 161s - loss: 3.4295 - accuracy: 0.3419 - 161s/epoch - 878us/step\n",
      "Epoch 25/30\n",
      "183063/183063 - 161s - loss: 3.4244 - accuracy: 0.3418 - 161s/epoch - 878us/step\n",
      "Epoch 26/30\n",
      "183063/183063 - 161s - loss: 3.4252 - accuracy: 0.3417 - 161s/epoch - 878us/step\n",
      "Epoch 27/30\n",
      "183063/183063 - 161s - loss: 3.4342 - accuracy: 0.3419 - 161s/epoch - 878us/step\n",
      "Epoch 28/30\n",
      "183063/183063 - 161s - loss: 3.4316 - accuracy: 0.3416 - 161s/epoch - 877us/step\n",
      "Epoch 29/30\n",
      "183063/183063 - 161s - loss: 3.4328 - accuracy: 0.3416 - 161s/epoch - 877us/step\n",
      "Epoch 30/30\n",
      "183063/183063 - 160s - loss: 3.4272 - accuracy: 0.3415 - 160s/epoch - 877us/step\n",
      "Epoch 1/30\n",
      "183063/183063 - 161s - loss: 3.3915 - accuracy: 0.3413 - 161s/epoch - 878us/step\n",
      "Epoch 2/30\n",
      "183063/183063 - 161s - loss: 3.3926 - accuracy: 0.3415 - 161s/epoch - 877us/step\n",
      "Epoch 3/30\n",
      "183063/183063 - 160s - loss: 3.3955 - accuracy: 0.3412 - 160s/epoch - 876us/step\n",
      "Epoch 4/30\n",
      "183063/183063 - 160s - loss: 3.3894 - accuracy: 0.3414 - 160s/epoch - 874us/step\n",
      "Epoch 5/30\n",
      "183063/183063 - 160s - loss: 3.3919 - accuracy: 0.3414 - 160s/epoch - 875us/step\n",
      "Epoch 6/30\n",
      "183063/183063 - 161s - loss: 3.3928 - accuracy: 0.3413 - 161s/epoch - 878us/step\n",
      "Epoch 7/30\n",
      "183063/183063 - 161s - loss: 3.3918 - accuracy: 0.3414 - 161s/epoch - 880us/step\n",
      "Epoch 8/30\n",
      "183063/183063 - 161s - loss: 3.3937 - accuracy: 0.3415 - 161s/epoch - 878us/step\n",
      "Epoch 9/30\n",
      "183063/183063 - 161s - loss: 3.3889 - accuracy: 0.3415 - 161s/epoch - 879us/step\n",
      "Epoch 10/30\n",
      "183063/183063 - 161s - loss: 3.3872 - accuracy: 0.3416 - 161s/epoch - 878us/step\n",
      "Epoch 11/30\n",
      "183063/183063 - 161s - loss: 3.3914 - accuracy: 0.3415 - 161s/epoch - 878us/step\n",
      "Epoch 12/30\n",
      "183063/183063 - 161s - loss: 3.3856 - accuracy: 0.3415 - 161s/epoch - 878us/step\n",
      "Epoch 13/30\n",
      "183063/183063 - 161s - loss: 3.3937 - accuracy: 0.3416 - 161s/epoch - 879us/step\n",
      "Epoch 14/30\n",
      "183063/183063 - 161s - loss: 3.3867 - accuracy: 0.3417 - 161s/epoch - 879us/step\n",
      "Epoch 15/30\n",
      "183063/183063 - 161s - loss: 3.3916 - accuracy: 0.3416 - 161s/epoch - 879us/step\n",
      "Epoch 16/30\n",
      "183063/183063 - 161s - loss: 3.3825 - accuracy: 0.3420 - 161s/epoch - 879us/step\n",
      "Epoch 17/30\n",
      "183063/183063 - 161s - loss: 3.3890 - accuracy: 0.3419 - 161s/epoch - 880us/step\n",
      "Epoch 18/30\n",
      "183063/183063 - 160s - loss: 3.3858 - accuracy: 0.3420 - 160s/epoch - 876us/step\n",
      "Epoch 19/30\n",
      "183063/183063 - 161s - loss: 3.3849 - accuracy: 0.3419 - 161s/epoch - 879us/step\n",
      "Epoch 20/30\n",
      "183063/183063 - 161s - loss: 3.3810 - accuracy: 0.3421 - 161s/epoch - 878us/step\n",
      "Epoch 21/30\n",
      "183063/183063 - 161s - loss: 3.3830 - accuracy: 0.3420 - 161s/epoch - 878us/step\n",
      "Epoch 22/30\n",
      "183063/183063 - 161s - loss: 3.3884 - accuracy: 0.3420 - 161s/epoch - 879us/step\n",
      "Epoch 23/30\n",
      "183063/183063 - 161s - loss: 3.3820 - accuracy: 0.3421 - 161s/epoch - 880us/step\n",
      "Epoch 24/30\n",
      "183063/183063 - 161s - loss: 3.3891 - accuracy: 0.3421 - 161s/epoch - 879us/step\n",
      "Epoch 25/30\n",
      "183063/183063 - 161s - loss: 3.3863 - accuracy: 0.3423 - 161s/epoch - 878us/step\n",
      "Epoch 26/30\n",
      "183063/183063 - 161s - loss: 3.3871 - accuracy: 0.3422 - 161s/epoch - 879us/step\n",
      "Epoch 27/30\n",
      "183063/183063 - 161s - loss: 3.3854 - accuracy: 0.3423 - 161s/epoch - 877us/step\n",
      "Epoch 28/30\n",
      "183063/183063 - 161s - loss: 3.3870 - accuracy: 0.3422 - 161s/epoch - 879us/step\n",
      "Epoch 29/30\n",
      "183063/183063 - 161s - loss: 3.3892 - accuracy: 0.3423 - 161s/epoch - 878us/step\n",
      "Epoch 30/30\n",
      "183063/183063 - 161s - loss: 3.3854 - accuracy: 0.3422 - 161s/epoch - 878us/step\n",
      "Epoch 1/30\n",
      "183063/183063 - 161s - loss: 3.5433 - accuracy: 0.3335 - 161s/epoch - 878us/step\n",
      "Epoch 2/30\n",
      "183063/183063 - 161s - loss: 3.5454 - accuracy: 0.3335 - 161s/epoch - 877us/step\n",
      "Epoch 3/30\n",
      "183063/183063 - 161s - loss: 3.5467 - accuracy: 0.3334 - 161s/epoch - 878us/step\n",
      "Epoch 4/30\n",
      "183063/183063 - 161s - loss: 3.5429 - accuracy: 0.3336 - 161s/epoch - 880us/step\n",
      "Epoch 5/30\n",
      "183063/183063 - 161s - loss: 3.5405 - accuracy: 0.3336 - 161s/epoch - 880us/step\n",
      "Epoch 6/30\n",
      "183063/183063 - 161s - loss: 3.5477 - accuracy: 0.3335 - 161s/epoch - 879us/step\n",
      "Epoch 7/30\n",
      "183063/183063 - 161s - loss: 3.5410 - accuracy: 0.3336 - 161s/epoch - 881us/step\n",
      "Epoch 8/30\n",
      "183063/183063 - 161s - loss: 3.5414 - accuracy: 0.3336 - 161s/epoch - 880us/step\n",
      "Epoch 9/30\n",
      "183063/183063 - 161s - loss: 3.5395 - accuracy: 0.3335 - 161s/epoch - 878us/step\n",
      "Epoch 10/30\n",
      "183063/183063 - 161s - loss: 3.5443 - accuracy: 0.3336 - 161s/epoch - 880us/step\n",
      "Epoch 11/30\n",
      "183063/183063 - 161s - loss: 3.5449 - accuracy: 0.3335 - 161s/epoch - 882us/step\n",
      "Epoch 12/30\n",
      "183063/183063 - 161s - loss: 3.5408 - accuracy: 0.3335 - 161s/epoch - 881us/step\n",
      "Epoch 13/30\n",
      "183063/183063 - 161s - loss: 3.5436 - accuracy: 0.3334 - 161s/epoch - 880us/step\n",
      "Epoch 14/30\n",
      "183063/183063 - 161s - loss: 3.5369 - accuracy: 0.3336 - 161s/epoch - 880us/step\n",
      "Epoch 15/30\n",
      "183063/183063 - 161s - loss: 3.5409 - accuracy: 0.3336 - 161s/epoch - 879us/step\n",
      "Epoch 16/30\n",
      "183063/183063 - 161s - loss: 3.5410 - accuracy: 0.3336 - 161s/epoch - 880us/step\n",
      "Epoch 17/30\n",
      "183063/183063 - 161s - loss: 3.5454 - accuracy: 0.3337 - 161s/epoch - 880us/step\n",
      "Epoch 18/30\n",
      "183063/183063 - 161s - loss: 3.5440 - accuracy: 0.3338 - 161s/epoch - 880us/step\n",
      "Epoch 19/30\n",
      "183063/183063 - 161s - loss: 3.5387 - accuracy: 0.3337 - 161s/epoch - 880us/step\n",
      "Epoch 20/30\n",
      "183063/183063 - 161s - loss: 3.5397 - accuracy: 0.3337 - 161s/epoch - 878us/step\n",
      "Epoch 21/30\n",
      "183063/183063 - 161s - loss: 3.5368 - accuracy: 0.3336 - 161s/epoch - 880us/step\n",
      "Epoch 22/30\n",
      "183063/183063 - 161s - loss: 3.5442 - accuracy: 0.3336 - 161s/epoch - 880us/step\n",
      "Epoch 23/30\n",
      "183063/183063 - 161s - loss: 3.5355 - accuracy: 0.3336 - 161s/epoch - 881us/step\n",
      "Epoch 24/30\n",
      "183063/183063 - 161s - loss: 3.5420 - accuracy: 0.3336 - 161s/epoch - 882us/step\n",
      "Epoch 25/30\n",
      "183063/183063 - 162s - loss: 3.5451 - accuracy: 0.3337 - 162s/epoch - 882us/step\n",
      "Epoch 26/30\n",
      "183063/183063 - 161s - loss: 3.5382 - accuracy: 0.3337 - 161s/epoch - 882us/step\n",
      "Epoch 27/30\n",
      "183063/183063 - 161s - loss: 3.5439 - accuracy: 0.3336 - 161s/epoch - 879us/step\n",
      "Epoch 28/30\n",
      "183063/183063 - 159s - loss: 3.5405 - accuracy: 0.3338 - 159s/epoch - 868us/step\n",
      "Epoch 29/30\n",
      "183063/183063 - 155s - loss: 3.5460 - accuracy: 0.3337 - 155s/epoch - 848us/step\n",
      "Epoch 30/30\n",
      "183063/183063 - 155s - loss: 3.5441 - accuracy: 0.3337 - 155s/epoch - 847us/step\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 2.193455696105957 - Accuracy: 45.08627951145172%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 3.9166059494018555 - Accuracy: 32.628682255744934%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 3.5763614177703857 - Accuracy: 34.428003430366516%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 3.03865122795105 - Accuracy: 39.046213030815125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 2.2893381118774414 - Accuracy: 41.848573088645935%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 3.9092817306518555 - Accuracy: 30.049163103103638%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.799490928649902 - Accuracy: 26.076695322990417%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.735569953918457 - Accuracy: 25.167158246040344%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.958621501922607 - Accuracy: 25.496557354927063%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 3.547903299331665 - Accuracy: 33.26942026615143%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 33.30967456102371 (+- 6.606946437409675)\n",
      "> Loss: 3.6965279817581176\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "met1 = dataset[[col for col in dataset.columns if col in cols]]\n",
    "\n",
    "X = met1.drop(columns=[\"Count\"])\n",
    "y = met1[\"Count\"]\n",
    "\n",
    "X = np.reshape(X.values, (X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=10, shuffle=False)\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "model_met1 = Sequential()\n",
    "model_met1.add(LSTM(4, input_shape=(1,met1.shape[1]-1)))\n",
    "model_met1.add(Dense(1))\n",
    "model_met1.compile(loss='mean_squared_error', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for train, test in kfold.split(X, y):\n",
    "    model_met1.fit(X[train], y[train], epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model_met1.evaluate(X[test], y[test], verbose=0)\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "mean = np.mean(loss_per_fold)\n",
    "print(f'> Loss: {mean}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
