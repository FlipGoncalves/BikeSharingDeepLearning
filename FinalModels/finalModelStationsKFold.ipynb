{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 15:50:57.210187: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-31 15:50:57.237328: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-31 15:50:57.237959: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-31 15:50:58.013350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../StationsDatasets/dataset.csv\").drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"StationEnd\", \"WorkingDay\", \"Hour\", \"Count\", \"Count1\", \"Count1week\", \"Count2week\", \"Count3week\", \"Temp\", \"Humidity\", \"WeatherSituation\", \"Windspeed\"]\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 1\n",
    "verbose = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 16:50:32.158500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-31 16:50:32.159197: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 16:50:32.590539: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-31 16:50:32.591789: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-31 16:50:32.592585: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-31 16:50:32.894244: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-31 16:50:32.895554: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-31 16:50:32.896392: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-31 16:50:33.241644: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-31 16:50:33.242925: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-31 16:50:33.243841: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182739/182739 - 163s - loss: 3.9129 - accuracy: 0.3223 - 163s/epoch - 895us/step\n",
      "Epoch 2/30\n",
      "182739/182739 - 158s - loss: 3.7029 - accuracy: 0.3230 - 158s/epoch - 865us/step\n",
      "Epoch 3/30\n",
      "182739/182739 - 158s - loss: 3.6684 - accuracy: 0.3230 - 158s/epoch - 865us/step\n",
      "Epoch 4/30\n",
      "182739/182739 - 159s - loss: 3.6491 - accuracy: 0.3230 - 159s/epoch - 868us/step\n",
      "Epoch 5/30\n",
      "182739/182739 - 159s - loss: 3.6293 - accuracy: 0.3229 - 159s/epoch - 868us/step\n",
      "Epoch 6/30\n",
      "182739/182739 - 158s - loss: 3.5949 - accuracy: 0.3224 - 158s/epoch - 867us/step\n",
      "Epoch 7/30\n",
      "182739/182739 - 160s - loss: 3.5745 - accuracy: 0.3221 - 160s/epoch - 873us/step\n",
      "Epoch 8/30\n",
      "182739/182739 - 156s - loss: 3.5588 - accuracy: 0.3223 - 156s/epoch - 854us/step\n",
      "Epoch 9/30\n",
      "182739/182739 - 158s - loss: 3.5443 - accuracy: 0.3222 - 158s/epoch - 864us/step\n",
      "Epoch 10/30\n",
      "182739/182739 - 158s - loss: 3.5413 - accuracy: 0.3221 - 158s/epoch - 864us/step\n",
      "Epoch 11/30\n",
      "182739/182739 - 158s - loss: 3.5322 - accuracy: 0.3219 - 158s/epoch - 864us/step\n",
      "Epoch 12/30\n",
      "182739/182739 - 158s - loss: 3.5333 - accuracy: 0.3218 - 158s/epoch - 866us/step\n",
      "Epoch 13/30\n",
      "182739/182739 - 158s - loss: 3.5306 - accuracy: 0.3218 - 158s/epoch - 865us/step\n",
      "Epoch 14/30\n",
      "182739/182739 - 158s - loss: 3.5269 - accuracy: 0.3219 - 158s/epoch - 864us/step\n",
      "Epoch 15/30\n",
      "182739/182739 - 158s - loss: 3.5202 - accuracy: 0.3221 - 158s/epoch - 866us/step\n",
      "Epoch 16/30\n",
      "182739/182739 - 158s - loss: 3.5225 - accuracy: 0.3222 - 158s/epoch - 865us/step\n",
      "Epoch 17/30\n",
      "182739/182739 - 158s - loss: 3.5223 - accuracy: 0.3224 - 158s/epoch - 863us/step\n",
      "Epoch 18/30\n",
      "182739/182739 - 158s - loss: 3.5248 - accuracy: 0.3225 - 158s/epoch - 866us/step\n",
      "Epoch 19/30\n",
      "182739/182739 - 158s - loss: 3.5173 - accuracy: 0.3224 - 158s/epoch - 864us/step\n",
      "Epoch 20/30\n",
      "182739/182739 - 158s - loss: 3.5174 - accuracy: 0.3225 - 158s/epoch - 867us/step\n",
      "Epoch 21/30\n",
      "182739/182739 - 158s - loss: 3.5191 - accuracy: 0.3225 - 158s/epoch - 865us/step\n",
      "Epoch 22/30\n",
      "182739/182739 - 158s - loss: 3.5201 - accuracy: 0.3226 - 158s/epoch - 865us/step\n",
      "Epoch 23/30\n",
      "182739/182739 - 159s - loss: 3.5156 - accuracy: 0.3225 - 159s/epoch - 868us/step\n",
      "Epoch 24/30\n",
      "182739/182739 - 158s - loss: 3.5181 - accuracy: 0.3226 - 158s/epoch - 865us/step\n",
      "Epoch 25/30\n",
      "182739/182739 - 158s - loss: 3.5201 - accuracy: 0.3225 - 158s/epoch - 866us/step\n",
      "Epoch 26/30\n",
      "182739/182739 - 158s - loss: 3.5184 - accuracy: 0.3225 - 158s/epoch - 865us/step\n",
      "Epoch 27/30\n",
      "182739/182739 - 158s - loss: 3.5200 - accuracy: 0.3225 - 158s/epoch - 866us/step\n",
      "Epoch 28/30\n",
      "182739/182739 - 158s - loss: 3.5133 - accuracy: 0.3226 - 158s/epoch - 864us/step\n",
      "Epoch 29/30\n",
      "182739/182739 - 158s - loss: 3.5157 - accuracy: 0.3227 - 158s/epoch - 864us/step\n",
      "Epoch 30/30\n",
      "182739/182739 - 158s - loss: 3.5130 - accuracy: 0.3225 - 158s/epoch - 864us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 18:09:41.811325: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-31 18:09:41.812756: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-31 18:09:41.813773: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "184815/184815 - 163s - loss: 3.4986 - accuracy: 0.3324 - 163s/epoch - 881us/step\n",
      "Epoch 2/30\n",
      "184815/184815 - 162s - loss: 3.4988 - accuracy: 0.3323 - 162s/epoch - 876us/step\n",
      "Epoch 3/30\n",
      "184815/184815 - 162s - loss: 3.4953 - accuracy: 0.3322 - 162s/epoch - 877us/step\n",
      "Epoch 4/30\n",
      "184815/184815 - 162s - loss: 3.4977 - accuracy: 0.3323 - 162s/epoch - 876us/step\n",
      "Epoch 5/30\n",
      "184815/184815 - 162s - loss: 3.4915 - accuracy: 0.3323 - 162s/epoch - 876us/step\n",
      "Epoch 6/30\n",
      "184815/184815 - 162s - loss: 3.4920 - accuracy: 0.3322 - 162s/epoch - 875us/step\n",
      "Epoch 7/30\n",
      "184815/184815 - 161s - loss: 3.4975 - accuracy: 0.3323 - 161s/epoch - 873us/step\n",
      "Epoch 8/30\n",
      "184815/184815 - 161s - loss: 3.4966 - accuracy: 0.3323 - 161s/epoch - 872us/step\n",
      "Epoch 9/30\n",
      "184815/184815 - 161s - loss: 3.4957 - accuracy: 0.3323 - 161s/epoch - 872us/step\n",
      "Epoch 10/30\n",
      "184815/184815 - 161s - loss: 3.4948 - accuracy: 0.3323 - 161s/epoch - 873us/step\n",
      "Epoch 11/30\n",
      "184815/184815 - 161s - loss: 3.4917 - accuracy: 0.3323 - 161s/epoch - 872us/step\n",
      "Epoch 12/30\n",
      "184815/184815 - 161s - loss: 3.4953 - accuracy: 0.3323 - 161s/epoch - 873us/step\n",
      "Epoch 13/30\n",
      "184815/184815 - 162s - loss: 3.4990 - accuracy: 0.3324 - 162s/epoch - 874us/step\n",
      "Epoch 14/30\n",
      "184815/184815 - 162s - loss: 3.4937 - accuracy: 0.3323 - 162s/epoch - 876us/step\n",
      "Epoch 15/30\n",
      "184815/184815 - 162s - loss: 3.4936 - accuracy: 0.3321 - 162s/epoch - 877us/step\n",
      "Epoch 16/30\n",
      "184815/184815 - 162s - loss: 3.4937 - accuracy: 0.3324 - 162s/epoch - 877us/step\n",
      "Epoch 17/30\n",
      "184815/184815 - 161s - loss: 3.4976 - accuracy: 0.3323 - 161s/epoch - 874us/step\n",
      "Epoch 18/30\n",
      "184815/184815 - 161s - loss: 3.4977 - accuracy: 0.3323 - 161s/epoch - 871us/step\n",
      "Epoch 19/30\n",
      "184815/184815 - 161s - loss: 3.4950 - accuracy: 0.3323 - 161s/epoch - 872us/step\n",
      "Epoch 20/30\n",
      "184815/184815 - 161s - loss: 3.4917 - accuracy: 0.3323 - 161s/epoch - 872us/step\n",
      "Epoch 21/30\n",
      "184815/184815 - 161s - loss: 3.4914 - accuracy: 0.3323 - 161s/epoch - 872us/step\n",
      "Epoch 22/30\n",
      "184815/184815 - 161s - loss: 3.4965 - accuracy: 0.3324 - 161s/epoch - 870us/step\n",
      "Epoch 23/30\n",
      "184815/184815 - 161s - loss: 3.4943 - accuracy: 0.3321 - 161s/epoch - 870us/step\n",
      "Epoch 24/30\n",
      "184815/184815 - 161s - loss: 3.4922 - accuracy: 0.3324 - 161s/epoch - 870us/step\n",
      "Epoch 25/30\n",
      "184815/184815 - 161s - loss: 3.4920 - accuracy: 0.3323 - 161s/epoch - 870us/step\n",
      "Epoch 26/30\n",
      "184815/184815 - 161s - loss: 3.4947 - accuracy: 0.3322 - 161s/epoch - 870us/step\n",
      "Epoch 27/30\n",
      "184815/184815 - 161s - loss: 3.4916 - accuracy: 0.3324 - 161s/epoch - 871us/step\n",
      "Epoch 28/30\n",
      "184815/184815 - 161s - loss: 3.4923 - accuracy: 0.3324 - 161s/epoch - 874us/step\n",
      "Epoch 29/30\n",
      "184815/184815 - 161s - loss: 3.4899 - accuracy: 0.3324 - 161s/epoch - 873us/step\n",
      "Epoch 30/30\n",
      "184815/184815 - 161s - loss: 3.4894 - accuracy: 0.3324 - 161s/epoch - 872us/step\n",
      "Epoch 1/30\n",
      "183211/183211 - 159s - loss: 3.5285 - accuracy: 0.3341 - 159s/epoch - 870us/step\n",
      "Epoch 2/30\n",
      "183211/183211 - 159s - loss: 3.5276 - accuracy: 0.3341 - 159s/epoch - 871us/step\n",
      "Epoch 3/30\n",
      "183211/183211 - 160s - loss: 3.5269 - accuracy: 0.3340 - 160s/epoch - 873us/step\n",
      "Epoch 4/30\n",
      "183211/183211 - 160s - loss: 3.5238 - accuracy: 0.3341 - 160s/epoch - 871us/step\n",
      "Epoch 5/30\n",
      "183211/183211 - 160s - loss: 3.5265 - accuracy: 0.3341 - 160s/epoch - 872us/step\n",
      "Epoch 6/30\n",
      "183211/183211 - 160s - loss: 3.5253 - accuracy: 0.3341 - 160s/epoch - 871us/step\n",
      "Epoch 7/30\n",
      "183211/183211 - 159s - loss: 3.5246 - accuracy: 0.3341 - 159s/epoch - 870us/step\n",
      "Epoch 8/30\n",
      "183211/183211 - 160s - loss: 3.5281 - accuracy: 0.3342 - 160s/epoch - 873us/step\n",
      "Epoch 9/30\n",
      "183211/183211 - 159s - loss: 3.5286 - accuracy: 0.3341 - 159s/epoch - 870us/step\n",
      "Epoch 10/30\n",
      "183211/183211 - 160s - loss: 3.5255 - accuracy: 0.3342 - 160s/epoch - 872us/step\n",
      "Epoch 11/30\n",
      "183211/183211 - 160s - loss: 3.5250 - accuracy: 0.3342 - 160s/epoch - 873us/step\n",
      "Epoch 12/30\n",
      "183211/183211 - 160s - loss: 3.5293 - accuracy: 0.3342 - 160s/epoch - 871us/step\n",
      "Epoch 13/30\n",
      "183211/183211 - 160s - loss: 3.5284 - accuracy: 0.3342 - 160s/epoch - 872us/step\n",
      "Epoch 14/30\n",
      "183211/183211 - 160s - loss: 3.5279 - accuracy: 0.3342 - 160s/epoch - 872us/step\n",
      "Epoch 15/30\n",
      "183211/183211 - 160s - loss: 3.5274 - accuracy: 0.3342 - 160s/epoch - 872us/step\n",
      "Epoch 16/30\n",
      "183211/183211 - 160s - loss: 3.5295 - accuracy: 0.3342 - 160s/epoch - 872us/step\n",
      "Epoch 17/30\n",
      "183211/183211 - 160s - loss: 3.5267 - accuracy: 0.3342 - 160s/epoch - 872us/step\n",
      "Epoch 18/30\n",
      "183211/183211 - 160s - loss: 3.5246 - accuracy: 0.3342 - 160s/epoch - 872us/step\n",
      "Epoch 19/30\n",
      "183211/183211 - 160s - loss: 3.5292 - accuracy: 0.3341 - 160s/epoch - 872us/step\n",
      "Epoch 20/30\n",
      "183211/183211 - 160s - loss: 3.5292 - accuracy: 0.3342 - 160s/epoch - 871us/step\n",
      "Epoch 21/30\n",
      "183211/183211 - 160s - loss: 3.5271 - accuracy: 0.3342 - 160s/epoch - 872us/step\n",
      "Epoch 22/30\n",
      "183211/183211 - 159s - loss: 3.5289 - accuracy: 0.3341 - 159s/epoch - 870us/step\n",
      "Epoch 23/30\n",
      "183211/183211 - 159s - loss: 3.5282 - accuracy: 0.3342 - 159s/epoch - 871us/step\n",
      "Epoch 24/30\n",
      "183211/183211 - 159s - loss: 3.5250 - accuracy: 0.3342 - 159s/epoch - 870us/step\n",
      "Epoch 25/30\n",
      "183211/183211 - 159s - loss: 3.5315 - accuracy: 0.3341 - 159s/epoch - 870us/step\n",
      "Epoch 26/30\n",
      "183211/183211 - 160s - loss: 3.5271 - accuracy: 0.3342 - 160s/epoch - 872us/step\n",
      "Epoch 27/30\n",
      "183211/183211 - 160s - loss: 3.5269 - accuracy: 0.3342 - 160s/epoch - 871us/step\n",
      "Epoch 28/30\n",
      "183211/183211 - 160s - loss: 3.5242 - accuracy: 0.3341 - 160s/epoch - 872us/step\n",
      "Epoch 29/30\n",
      "183211/183211 - 160s - loss: 3.5253 - accuracy: 0.3342 - 160s/epoch - 873us/step\n",
      "Epoch 30/30\n",
      "183211/183211 - 159s - loss: 3.5265 - accuracy: 0.3342 - 159s/epoch - 870us/step\n",
      "Epoch 1/30\n",
      "182778/182778 - 160s - loss: 3.5245 - accuracy: 0.3368 - 160s/epoch - 873us/step\n",
      "Epoch 2/30\n",
      "182778/182778 - 159s - loss: 3.5252 - accuracy: 0.3367 - 159s/epoch - 870us/step\n",
      "Epoch 3/30\n",
      "182778/182778 - 159s - loss: 3.5246 - accuracy: 0.3367 - 159s/epoch - 872us/step\n",
      "Epoch 4/30\n",
      "182778/182778 - 159s - loss: 3.5254 - accuracy: 0.3367 - 159s/epoch - 872us/step\n",
      "Epoch 5/30\n",
      "182778/182778 - 159s - loss: 3.5229 - accuracy: 0.3367 - 159s/epoch - 870us/step\n",
      "Epoch 6/30\n",
      "182778/182778 - 160s - loss: 3.5206 - accuracy: 0.3368 - 160s/epoch - 874us/step\n",
      "Epoch 7/30\n",
      "182778/182778 - 160s - loss: 3.5188 - accuracy: 0.3366 - 160s/epoch - 873us/step\n",
      "Epoch 8/30\n",
      "182778/182778 - 160s - loss: 3.5205 - accuracy: 0.3367 - 160s/epoch - 873us/step\n",
      "Epoch 9/30\n",
      "182778/182778 - 159s - loss: 3.5205 - accuracy: 0.3368 - 159s/epoch - 873us/step\n",
      "Epoch 10/30\n",
      "182778/182778 - 159s - loss: 3.5221 - accuracy: 0.3367 - 159s/epoch - 869us/step\n",
      "Epoch 11/30\n",
      "182778/182778 - 159s - loss: 3.5196 - accuracy: 0.3368 - 159s/epoch - 871us/step\n",
      "Epoch 12/30\n",
      "182778/182778 - 159s - loss: 3.5233 - accuracy: 0.3367 - 159s/epoch - 872us/step\n",
      "Epoch 13/30\n",
      "182778/182778 - 159s - loss: 3.5175 - accuracy: 0.3368 - 159s/epoch - 872us/step\n",
      "Epoch 14/30\n",
      "182778/182778 - 159s - loss: 3.5163 - accuracy: 0.3367 - 159s/epoch - 871us/step\n",
      "Epoch 15/30\n",
      "182778/182778 - 159s - loss: 3.5229 - accuracy: 0.3367 - 159s/epoch - 870us/step\n",
      "Epoch 16/30\n",
      "182778/182778 - 159s - loss: 3.5189 - accuracy: 0.3368 - 159s/epoch - 869us/step\n",
      "Epoch 17/30\n",
      "182778/182778 - 158s - loss: 3.5230 - accuracy: 0.3367 - 158s/epoch - 867us/step\n",
      "Epoch 18/30\n",
      "182778/182778 - 159s - loss: 3.5229 - accuracy: 0.3367 - 159s/epoch - 870us/step\n",
      "Epoch 19/30\n",
      "182778/182778 - 159s - loss: 3.5193 - accuracy: 0.3367 - 159s/epoch - 870us/step\n",
      "Epoch 20/30\n",
      "182778/182778 - 160s - loss: 3.5182 - accuracy: 0.3367 - 160s/epoch - 874us/step\n",
      "Epoch 21/30\n",
      "182778/182778 - 159s - loss: 3.5171 - accuracy: 0.3367 - 159s/epoch - 871us/step\n",
      "Epoch 22/30\n",
      "182778/182778 - 159s - loss: 3.5224 - accuracy: 0.3367 - 159s/epoch - 873us/step\n",
      "Epoch 23/30\n",
      "182778/182778 - 160s - loss: 3.5224 - accuracy: 0.3367 - 160s/epoch - 874us/step\n",
      "Epoch 24/30\n",
      "182778/182778 - 160s - loss: 3.5226 - accuracy: 0.3367 - 160s/epoch - 873us/step\n",
      "Epoch 25/30\n",
      "182778/182778 - 160s - loss: 3.5254 - accuracy: 0.3367 - 160s/epoch - 876us/step\n",
      "Epoch 26/30\n",
      "182778/182778 - 159s - loss: 3.5206 - accuracy: 0.3368 - 159s/epoch - 871us/step\n",
      "Epoch 27/30\n",
      "182778/182778 - 159s - loss: 3.5227 - accuracy: 0.3367 - 159s/epoch - 871us/step\n",
      "Epoch 28/30\n",
      "182778/182778 - 159s - loss: 3.5222 - accuracy: 0.3367 - 159s/epoch - 870us/step\n",
      "Epoch 29/30\n",
      "182778/182778 - 159s - loss: 3.5195 - accuracy: 0.3367 - 159s/epoch - 869us/step\n",
      "Epoch 30/30\n",
      "182778/182778 - 159s - loss: 3.5173 - accuracy: 0.3367 - 159s/epoch - 869us/step\n",
      "Epoch 1/30\n",
      "182903/182903 - 160s - loss: 3.5435 - accuracy: 0.3352 - 160s/epoch - 872us/step\n",
      "Epoch 2/30\n",
      "182903/182903 - 159s - loss: 3.5475 - accuracy: 0.3353 - 159s/epoch - 871us/step\n",
      "Epoch 3/30\n",
      "182903/182903 - 160s - loss: 3.5551 - accuracy: 0.3351 - 160s/epoch - 873us/step\n",
      "Epoch 4/30\n",
      "182903/182903 - 159s - loss: 3.5525 - accuracy: 0.3351 - 159s/epoch - 871us/step\n",
      "Epoch 5/30\n",
      "182903/182903 - 159s - loss: 3.5463 - accuracy: 0.3350 - 159s/epoch - 871us/step\n",
      "Epoch 6/30\n",
      "182903/182903 - 159s - loss: 3.5452 - accuracy: 0.3352 - 159s/epoch - 871us/step\n",
      "Epoch 7/30\n",
      "182903/182903 - 159s - loss: 3.5517 - accuracy: 0.3352 - 159s/epoch - 870us/step\n",
      "Epoch 8/30\n",
      "182903/182903 - 159s - loss: 3.5498 - accuracy: 0.3351 - 159s/epoch - 870us/step\n",
      "Epoch 9/30\n",
      "182903/182903 - 159s - loss: 3.5537 - accuracy: 0.3352 - 159s/epoch - 870us/step\n",
      "Epoch 10/30\n",
      "182903/182903 - 159s - loss: 3.5476 - accuracy: 0.3352 - 159s/epoch - 869us/step\n",
      "Epoch 11/30\n",
      "182903/182903 - 159s - loss: 3.5488 - accuracy: 0.3352 - 159s/epoch - 869us/step\n",
      "Epoch 12/30\n",
      "182903/182903 - 159s - loss: 3.5437 - accuracy: 0.3351 - 159s/epoch - 871us/step\n",
      "Epoch 13/30\n",
      "182903/182903 - 159s - loss: 3.5454 - accuracy: 0.3352 - 159s/epoch - 868us/step\n",
      "Epoch 14/30\n",
      "182903/182903 - 159s - loss: 3.5490 - accuracy: 0.3350 - 159s/epoch - 869us/step\n",
      "Epoch 15/30\n",
      "182903/182903 - 159s - loss: 3.5453 - accuracy: 0.3353 - 159s/epoch - 868us/step\n",
      "Epoch 16/30\n",
      "182903/182903 - 159s - loss: 3.5487 - accuracy: 0.3351 - 159s/epoch - 870us/step\n",
      "Epoch 17/30\n",
      "182903/182903 - 159s - loss: 3.5532 - accuracy: 0.3352 - 159s/epoch - 870us/step\n",
      "Epoch 18/30\n",
      "182903/182903 - 158s - loss: 3.5467 - accuracy: 0.3351 - 158s/epoch - 866us/step\n",
      "Epoch 19/30\n",
      "182903/182903 - 159s - loss: 3.5444 - accuracy: 0.3351 - 159s/epoch - 868us/step\n",
      "Epoch 20/30\n",
      "182903/182903 - 159s - loss: 3.5487 - accuracy: 0.3352 - 159s/epoch - 869us/step\n",
      "Epoch 21/30\n",
      "182903/182903 - 159s - loss: 3.5474 - accuracy: 0.3351 - 159s/epoch - 869us/step\n",
      "Epoch 22/30\n",
      "182903/182903 - 159s - loss: 3.5422 - accuracy: 0.3351 - 159s/epoch - 868us/step\n",
      "Epoch 23/30\n",
      "182903/182903 - 159s - loss: 3.5434 - accuracy: 0.3352 - 159s/epoch - 869us/step\n",
      "Epoch 24/30\n",
      "182903/182903 - 159s - loss: 3.5494 - accuracy: 0.3352 - 159s/epoch - 869us/step\n",
      "Epoch 25/30\n",
      "182903/182903 - 159s - loss: 3.5442 - accuracy: 0.3352 - 159s/epoch - 870us/step\n",
      "Epoch 26/30\n",
      "182903/182903 - 159s - loss: 3.5479 - accuracy: 0.3352 - 159s/epoch - 870us/step\n",
      "Epoch 27/30\n",
      "182903/182903 - 159s - loss: 3.5488 - accuracy: 0.3352 - 159s/epoch - 870us/step\n",
      "Epoch 28/30\n",
      "182903/182903 - 159s - loss: 3.5438 - accuracy: 0.3352 - 159s/epoch - 870us/step\n",
      "Epoch 29/30\n",
      "182903/182903 - 159s - loss: 3.5488 - accuracy: 0.3352 - 159s/epoch - 869us/step\n",
      "Epoch 30/30\n",
      "182903/182903 - 159s - loss: 3.5458 - accuracy: 0.3352 - 159s/epoch - 867us/step\n",
      "Epoch 1/30\n",
      "183284/183284 - 160s - loss: 3.6219 - accuracy: 0.3330 - 160s/epoch - 871us/step\n",
      "Epoch 2/30\n",
      "183284/183284 - 160s - loss: 3.6202 - accuracy: 0.3331 - 160s/epoch - 872us/step\n",
      "Epoch 3/30\n",
      "183284/183284 - 160s - loss: 3.6186 - accuracy: 0.3331 - 160s/epoch - 873us/step\n",
      "Epoch 4/30\n",
      "183284/183284 - 160s - loss: 3.6181 - accuracy: 0.3331 - 160s/epoch - 874us/step\n",
      "Epoch 5/30\n",
      "183284/183284 - 160s - loss: 3.6192 - accuracy: 0.3332 - 160s/epoch - 873us/step\n",
      "Epoch 6/30\n",
      "183284/183284 - 160s - loss: 3.6237 - accuracy: 0.3332 - 160s/epoch - 873us/step\n",
      "Epoch 7/30\n",
      "183284/183284 - 160s - loss: 3.6183 - accuracy: 0.3331 - 160s/epoch - 874us/step\n",
      "Epoch 8/30\n",
      "183284/183284 - 160s - loss: 3.6175 - accuracy: 0.3331 - 160s/epoch - 871us/step\n",
      "Epoch 9/30\n",
      "183284/183284 - 160s - loss: 3.6181 - accuracy: 0.3331 - 160s/epoch - 874us/step\n",
      "Epoch 10/30\n",
      "183284/183284 - 160s - loss: 3.6254 - accuracy: 0.3331 - 160s/epoch - 872us/step\n",
      "Epoch 11/30\n",
      "183284/183284 - 160s - loss: 3.6158 - accuracy: 0.3332 - 160s/epoch - 871us/step\n",
      "Epoch 12/30\n",
      "183284/183284 - 159s - loss: 3.6188 - accuracy: 0.3332 - 159s/epoch - 869us/step\n",
      "Epoch 13/30\n",
      "183284/183284 - 160s - loss: 3.6122 - accuracy: 0.3331 - 160s/epoch - 872us/step\n",
      "Epoch 14/30\n",
      "183284/183284 - 160s - loss: 3.6191 - accuracy: 0.3332 - 160s/epoch - 871us/step\n",
      "Epoch 15/30\n",
      "183284/183284 - 160s - loss: 3.6150 - accuracy: 0.3331 - 160s/epoch - 872us/step\n",
      "Epoch 16/30\n",
      "183284/183284 - 159s - loss: 3.6180 - accuracy: 0.3331 - 159s/epoch - 870us/step\n",
      "Epoch 17/30\n",
      "183284/183284 - 159s - loss: 3.6159 - accuracy: 0.3332 - 159s/epoch - 868us/step\n",
      "Epoch 18/30\n",
      "183284/183284 - 159s - loss: 3.6168 - accuracy: 0.3331 - 159s/epoch - 870us/step\n",
      "Epoch 19/30\n",
      "183284/183284 - 160s - loss: 3.6188 - accuracy: 0.3331 - 160s/epoch - 872us/step\n",
      "Epoch 20/30\n",
      "183284/183284 - 160s - loss: 3.6188 - accuracy: 0.3332 - 160s/epoch - 871us/step\n",
      "Epoch 21/30\n",
      "183284/183284 - 160s - loss: 3.6209 - accuracy: 0.3332 - 160s/epoch - 873us/step\n",
      "Epoch 22/30\n",
      "183284/183284 - 159s - loss: 3.6220 - accuracy: 0.3332 - 159s/epoch - 870us/step\n",
      "Epoch 23/30\n",
      "183284/183284 - 160s - loss: 3.6222 - accuracy: 0.3333 - 160s/epoch - 873us/step\n",
      "Epoch 24/30\n",
      "183284/183284 - 159s - loss: 3.6190 - accuracy: 0.3333 - 159s/epoch - 867us/step\n",
      "Epoch 25/30\n",
      "183284/183284 - 159s - loss: 3.6204 - accuracy: 0.3333 - 159s/epoch - 869us/step\n",
      "Epoch 26/30\n",
      "183284/183284 - 160s - loss: 3.6152 - accuracy: 0.3332 - 160s/epoch - 875us/step\n",
      "Epoch 27/30\n",
      "183284/183284 - 160s - loss: 3.6220 - accuracy: 0.3333 - 160s/epoch - 873us/step\n",
      "Epoch 28/30\n",
      "183284/183284 - 160s - loss: 3.6190 - accuracy: 0.3333 - 160s/epoch - 872us/step\n",
      "Epoch 29/30\n",
      "183284/183284 - 160s - loss: 3.6175 - accuracy: 0.3333 - 160s/epoch - 872us/step\n",
      "Epoch 30/30\n",
      "183284/183284 - 160s - loss: 3.6219 - accuracy: 0.3333 - 160s/epoch - 873us/step\n",
      "Epoch 1/30\n",
      "182908/182908 - 159s - loss: 3.6274 - accuracy: 0.3343 - 159s/epoch - 871us/step\n",
      "Epoch 2/30\n",
      "182908/182908 - 159s - loss: 3.6219 - accuracy: 0.3343 - 159s/epoch - 871us/step\n",
      "Epoch 3/30\n",
      "182908/182908 - 160s - loss: 3.6228 - accuracy: 0.3343 - 160s/epoch - 872us/step\n",
      "Epoch 4/30\n",
      "182908/182908 - 159s - loss: 3.6198 - accuracy: 0.3343 - 159s/epoch - 870us/step\n",
      "Epoch 5/30\n",
      "182908/182908 - 159s - loss: 3.6203 - accuracy: 0.3343 - 159s/epoch - 868us/step\n",
      "Epoch 6/30\n",
      "182908/182908 - 159s - loss: 3.6262 - accuracy: 0.3342 - 159s/epoch - 869us/step\n",
      "Epoch 7/30\n",
      "182908/182908 - 159s - loss: 3.6235 - accuracy: 0.3344 - 159s/epoch - 871us/step\n",
      "Epoch 8/30\n",
      "182908/182908 - 160s - loss: 3.6289 - accuracy: 0.3343 - 160s/epoch - 872us/step\n",
      "Epoch 9/30\n",
      "182908/182908 - 160s - loss: 3.6203 - accuracy: 0.3342 - 160s/epoch - 872us/step\n",
      "Epoch 10/30\n",
      "182908/182908 - 160s - loss: 3.6216 - accuracy: 0.3342 - 160s/epoch - 873us/step\n",
      "Epoch 11/30\n",
      "182908/182908 - 159s - loss: 3.6268 - accuracy: 0.3343 - 159s/epoch - 871us/step\n",
      "Epoch 12/30\n",
      "182908/182908 - 159s - loss: 3.6267 - accuracy: 0.3343 - 159s/epoch - 872us/step\n",
      "Epoch 13/30\n",
      "182908/182908 - 159s - loss: 3.6234 - accuracy: 0.3343 - 159s/epoch - 870us/step\n",
      "Epoch 14/30\n",
      "182908/182908 - 159s - loss: 3.6257 - accuracy: 0.3343 - 159s/epoch - 871us/step\n",
      "Epoch 15/30\n",
      "182908/182908 - 159s - loss: 3.6207 - accuracy: 0.3343 - 159s/epoch - 868us/step\n",
      "Epoch 16/30\n",
      "182908/182908 - 159s - loss: 3.6232 - accuracy: 0.3343 - 159s/epoch - 867us/step\n",
      "Epoch 17/30\n",
      "182908/182908 - 159s - loss: 3.6239 - accuracy: 0.3342 - 159s/epoch - 869us/step\n",
      "Epoch 18/30\n",
      "182908/182908 - 159s - loss: 3.6293 - accuracy: 0.3343 - 159s/epoch - 870us/step\n",
      "Epoch 19/30\n",
      "182908/182908 - 159s - loss: 3.6240 - accuracy: 0.3342 - 159s/epoch - 868us/step\n",
      "Epoch 20/30\n",
      "182908/182908 - 159s - loss: 3.6220 - accuracy: 0.3342 - 159s/epoch - 868us/step\n",
      "Epoch 21/30\n",
      "182908/182908 - 159s - loss: 3.6267 - accuracy: 0.3342 - 159s/epoch - 870us/step\n",
      "Epoch 22/30\n",
      "182908/182908 - 159s - loss: 3.6264 - accuracy: 0.3343 - 159s/epoch - 869us/step\n",
      "Epoch 23/30\n",
      "182908/182908 - 159s - loss: 3.6237 - accuracy: 0.3343 - 159s/epoch - 870us/step\n",
      "Epoch 24/30\n",
      "182908/182908 - 159s - loss: 3.6223 - accuracy: 0.3343 - 159s/epoch - 868us/step\n",
      "Epoch 25/30\n",
      "182908/182908 - 159s - loss: 3.6227 - accuracy: 0.3343 - 159s/epoch - 868us/step\n",
      "Epoch 26/30\n",
      "182908/182908 - 159s - loss: 3.6181 - accuracy: 0.3343 - 159s/epoch - 870us/step\n",
      "Epoch 27/30\n",
      "182908/182908 - 159s - loss: 3.6223 - accuracy: 0.3342 - 159s/epoch - 869us/step\n",
      "Epoch 28/30\n",
      "182908/182908 - 159s - loss: 3.6273 - accuracy: 0.3343 - 159s/epoch - 872us/step\n",
      "Epoch 29/30\n",
      "182908/182908 - 160s - loss: 3.6228 - accuracy: 0.3343 - 160s/epoch - 874us/step\n",
      "Epoch 30/30\n",
      "182908/182908 - 160s - loss: 3.6183 - accuracy: 0.3343 - 160s/epoch - 873us/step\n",
      "Epoch 1/30\n",
      "182892/182892 - 160s - loss: 3.6335 - accuracy: 0.3350 - 160s/epoch - 873us/step\n",
      "Epoch 2/30\n",
      "182892/182892 - 160s - loss: 3.6300 - accuracy: 0.3351 - 160s/epoch - 874us/step\n",
      "Epoch 3/30\n",
      "182892/182892 - 159s - loss: 3.6270 - accuracy: 0.3352 - 159s/epoch - 872us/step\n",
      "Epoch 4/30\n",
      "182892/182892 - 159s - loss: 3.6298 - accuracy: 0.3351 - 159s/epoch - 872us/step\n",
      "Epoch 5/30\n",
      "182892/182892 - 159s - loss: 3.6347 - accuracy: 0.3351 - 159s/epoch - 872us/step\n",
      "Epoch 6/30\n",
      "182892/182892 - 160s - loss: 3.6292 - accuracy: 0.3350 - 160s/epoch - 873us/step\n",
      "Epoch 7/30\n",
      "182892/182892 - 160s - loss: 3.6348 - accuracy: 0.3351 - 160s/epoch - 872us/step\n",
      "Epoch 8/30\n",
      "182892/182892 - 160s - loss: 3.6337 - accuracy: 0.3351 - 160s/epoch - 872us/step\n",
      "Epoch 9/30\n",
      "182892/182892 - 159s - loss: 3.6268 - accuracy: 0.3351 - 159s/epoch - 871us/step\n",
      "Epoch 10/30\n",
      "182892/182892 - 159s - loss: 3.6299 - accuracy: 0.3351 - 159s/epoch - 869us/step\n",
      "Epoch 11/30\n",
      "182892/182892 - 159s - loss: 3.6315 - accuracy: 0.3352 - 159s/epoch - 869us/step\n",
      "Epoch 12/30\n",
      "182892/182892 - 159s - loss: 3.6259 - accuracy: 0.3352 - 159s/epoch - 871us/step\n",
      "Epoch 13/30\n",
      "182892/182892 - 159s - loss: 3.6295 - accuracy: 0.3351 - 159s/epoch - 870us/step\n",
      "Epoch 14/30\n",
      "182892/182892 - 159s - loss: 3.6238 - accuracy: 0.3352 - 159s/epoch - 870us/step\n",
      "Epoch 15/30\n",
      "182892/182892 - 160s - loss: 3.6273 - accuracy: 0.3351 - 160s/epoch - 874us/step\n",
      "Epoch 16/30\n",
      "182892/182892 - 159s - loss: 3.6249 - accuracy: 0.3351 - 159s/epoch - 872us/step\n",
      "Epoch 17/30\n",
      "182892/182892 - 159s - loss: 3.6270 - accuracy: 0.3351 - 159s/epoch - 870us/step\n",
      "Epoch 18/30\n",
      "182892/182892 - 160s - loss: 3.6300 - accuracy: 0.3350 - 160s/epoch - 873us/step\n",
      "Epoch 19/30\n",
      "182892/182892 - 159s - loss: 3.6295 - accuracy: 0.3350 - 159s/epoch - 871us/step\n",
      "Epoch 20/30\n",
      "182892/182892 - 159s - loss: 3.6299 - accuracy: 0.3351 - 159s/epoch - 871us/step\n",
      "Epoch 21/30\n",
      "182892/182892 - 160s - loss: 3.6315 - accuracy: 0.3351 - 160s/epoch - 873us/step\n",
      "Epoch 22/30\n",
      "182892/182892 - 159s - loss: 3.6257 - accuracy: 0.3351 - 159s/epoch - 871us/step\n",
      "Epoch 23/30\n",
      "182892/182892 - 160s - loss: 3.6289 - accuracy: 0.3351 - 160s/epoch - 873us/step\n",
      "Epoch 24/30\n",
      "182892/182892 - 159s - loss: 3.6294 - accuracy: 0.3351 - 159s/epoch - 871us/step\n",
      "Epoch 25/30\n",
      "182892/182892 - 159s - loss: 3.6277 - accuracy: 0.3350 - 159s/epoch - 871us/step\n",
      "Epoch 26/30\n",
      "182892/182892 - 160s - loss: 3.6294 - accuracy: 0.3352 - 160s/epoch - 873us/step\n",
      "Epoch 27/30\n",
      "182892/182892 - 159s - loss: 3.6298 - accuracy: 0.3351 - 159s/epoch - 872us/step\n",
      "Epoch 28/30\n",
      "182892/182892 - 159s - loss: 3.6233 - accuracy: 0.3350 - 159s/epoch - 870us/step\n",
      "Epoch 29/30\n",
      "182892/182892 - 159s - loss: 3.6317 - accuracy: 0.3351 - 159s/epoch - 871us/step\n",
      "Epoch 30/30\n",
      "182892/182892 - 159s - loss: 3.6273 - accuracy: 0.3351 - 159s/epoch - 870us/step\n",
      "Epoch 1/30\n",
      "182676/182676 - 159s - loss: 3.6347 - accuracy: 0.3347 - 159s/epoch - 869us/step\n",
      "Epoch 2/30\n",
      "182676/182676 - 159s - loss: 3.6352 - accuracy: 0.3347 - 159s/epoch - 871us/step\n",
      "Epoch 3/30\n",
      "182676/182676 - 159s - loss: 3.6295 - accuracy: 0.3346 - 159s/epoch - 869us/step\n",
      "Epoch 4/30\n",
      "182676/182676 - 159s - loss: 3.6309 - accuracy: 0.3347 - 159s/epoch - 868us/step\n",
      "Epoch 5/30\n",
      "182676/182676 - 159s - loss: 3.6320 - accuracy: 0.3347 - 159s/epoch - 869us/step\n",
      "Epoch 6/30\n",
      "182676/182676 - 159s - loss: 3.6314 - accuracy: 0.3347 - 159s/epoch - 868us/step\n",
      "Epoch 7/30\n",
      "182676/182676 - 159s - loss: 3.6327 - accuracy: 0.3347 - 159s/epoch - 870us/step\n",
      "Epoch 8/30\n",
      "182676/182676 - 159s - loss: 3.6312 - accuracy: 0.3346 - 159s/epoch - 872us/step\n",
      "Epoch 9/30\n",
      "182676/182676 - 159s - loss: 3.6321 - accuracy: 0.3347 - 159s/epoch - 870us/step\n",
      "Epoch 10/30\n",
      "182676/182676 - 159s - loss: 3.6321 - accuracy: 0.3347 - 159s/epoch - 870us/step\n",
      "Epoch 11/30\n",
      "182676/182676 - 159s - loss: 3.6294 - accuracy: 0.3347 - 159s/epoch - 871us/step\n",
      "Epoch 12/30\n",
      "182676/182676 - 159s - loss: 3.6358 - accuracy: 0.3347 - 159s/epoch - 871us/step\n",
      "Epoch 13/30\n",
      "182676/182676 - 159s - loss: 3.6322 - accuracy: 0.3347 - 159s/epoch - 870us/step\n",
      "Epoch 14/30\n",
      "182676/182676 - 159s - loss: 3.6329 - accuracy: 0.3347 - 159s/epoch - 870us/step\n",
      "Epoch 15/30\n",
      "182676/182676 - 159s - loss: 3.6346 - accuracy: 0.3346 - 159s/epoch - 872us/step\n",
      "Epoch 16/30\n",
      "182676/182676 - 159s - loss: 3.6350 - accuracy: 0.3347 - 159s/epoch - 870us/step\n",
      "Epoch 17/30\n",
      "182676/182676 - 159s - loss: 3.6287 - accuracy: 0.3347 - 159s/epoch - 870us/step\n",
      "Epoch 18/30\n",
      "182676/182676 - 159s - loss: 3.6346 - accuracy: 0.3346 - 159s/epoch - 872us/step\n",
      "Epoch 19/30\n",
      "182676/182676 - 159s - loss: 3.6371 - accuracy: 0.3347 - 159s/epoch - 872us/step\n",
      "Epoch 20/30\n",
      "182676/182676 - 159s - loss: 3.6343 - accuracy: 0.3347 - 159s/epoch - 869us/step\n",
      "Epoch 21/30\n",
      "182676/182676 - 159s - loss: 3.6352 - accuracy: 0.3348 - 159s/epoch - 871us/step\n",
      "Epoch 22/30\n",
      "182676/182676 - 159s - loss: 3.6318 - accuracy: 0.3346 - 159s/epoch - 871us/step\n",
      "Epoch 23/30\n",
      "182676/182676 - 159s - loss: 3.6299 - accuracy: 0.3347 - 159s/epoch - 870us/step\n",
      "Epoch 24/30\n",
      "182676/182676 - 159s - loss: 3.6331 - accuracy: 0.3346 - 159s/epoch - 871us/step\n",
      "Epoch 25/30\n",
      "182676/182676 - 159s - loss: 3.6286 - accuracy: 0.3346 - 159s/epoch - 870us/step\n",
      "Epoch 26/30\n",
      "182676/182676 - 159s - loss: 3.6300 - accuracy: 0.3347 - 159s/epoch - 870us/step\n",
      "Epoch 27/30\n",
      "182676/182676 - 159s - loss: 3.6302 - accuracy: 0.3346 - 159s/epoch - 870us/step\n",
      "Epoch 28/30\n",
      "182676/182676 - 159s - loss: 3.6290 - accuracy: 0.3347 - 159s/epoch - 871us/step\n",
      "Epoch 29/30\n",
      "182676/182676 - 159s - loss: 3.6363 - accuracy: 0.3347 - 159s/epoch - 870us/step\n",
      "Epoch 30/30\n",
      "182676/182676 - 159s - loss: 3.6367 - accuracy: 0.3347 - 159s/epoch - 871us/step\n",
      "Epoch 1/30\n",
      "182421/182421 - 159s - loss: 3.6186 - accuracy: 0.3363 - 159s/epoch - 871us/step\n",
      "Epoch 2/30\n",
      "182421/182421 - 159s - loss: 3.6171 - accuracy: 0.3362 - 159s/epoch - 873us/step\n",
      "Epoch 3/30\n",
      "182421/182421 - 159s - loss: 3.6179 - accuracy: 0.3363 - 159s/epoch - 872us/step\n",
      "Epoch 4/30\n",
      "182421/182421 - 159s - loss: 3.6178 - accuracy: 0.3363 - 159s/epoch - 871us/step\n",
      "Epoch 5/30\n",
      "182421/182421 - 159s - loss: 3.6215 - accuracy: 0.3363 - 159s/epoch - 871us/step\n",
      "Epoch 6/30\n",
      "182421/182421 - 159s - loss: 3.6195 - accuracy: 0.3362 - 159s/epoch - 873us/step\n",
      "Epoch 7/30\n",
      "182421/182421 - 159s - loss: 3.6189 - accuracy: 0.3363 - 159s/epoch - 872us/step\n",
      "Epoch 8/30\n",
      "182421/182421 - 159s - loss: 3.6186 - accuracy: 0.3362 - 159s/epoch - 873us/step\n",
      "Epoch 9/30\n",
      "182421/182421 - 159s - loss: 3.6122 - accuracy: 0.3362 - 159s/epoch - 874us/step\n",
      "Epoch 10/30\n",
      "182421/182421 - 159s - loss: 3.6221 - accuracy: 0.3363 - 159s/epoch - 872us/step\n",
      "Epoch 11/30\n",
      "182421/182421 - 159s - loss: 3.6213 - accuracy: 0.3363 - 159s/epoch - 871us/step\n",
      "Epoch 12/30\n",
      "182421/182421 - 159s - loss: 3.6193 - accuracy: 0.3363 - 159s/epoch - 872us/step\n",
      "Epoch 13/30\n",
      "182421/182421 - 159s - loss: 3.6232 - accuracy: 0.3363 - 159s/epoch - 874us/step\n",
      "Epoch 14/30\n",
      "182421/182421 - 159s - loss: 3.6133 - accuracy: 0.3362 - 159s/epoch - 874us/step\n",
      "Epoch 15/30\n",
      "182421/182421 - 159s - loss: 3.6170 - accuracy: 0.3363 - 159s/epoch - 874us/step\n",
      "Epoch 16/30\n",
      "182421/182421 - 159s - loss: 3.6218 - accuracy: 0.3363 - 159s/epoch - 871us/step\n",
      "Epoch 17/30\n",
      "182421/182421 - 159s - loss: 3.6172 - accuracy: 0.3363 - 159s/epoch - 871us/step\n",
      "Epoch 18/30\n",
      "182421/182421 - 159s - loss: 3.6218 - accuracy: 0.3363 - 159s/epoch - 873us/step\n",
      "Epoch 19/30\n",
      "182421/182421 - 160s - loss: 3.6186 - accuracy: 0.3364 - 160s/epoch - 875us/step\n",
      "Epoch 20/30\n",
      "182421/182421 - 159s - loss: 3.6183 - accuracy: 0.3362 - 159s/epoch - 873us/step\n",
      "Epoch 21/30\n",
      "182421/182421 - 159s - loss: 3.6184 - accuracy: 0.3363 - 159s/epoch - 871us/step\n",
      "Epoch 22/30\n",
      "182421/182421 - 159s - loss: 3.6196 - accuracy: 0.3362 - 159s/epoch - 872us/step\n",
      "Epoch 23/30\n",
      "182421/182421 - 159s - loss: 3.6220 - accuracy: 0.3363 - 159s/epoch - 874us/step\n",
      "Epoch 24/30\n",
      "182421/182421 - 159s - loss: 3.6195 - accuracy: 0.3364 - 159s/epoch - 874us/step\n",
      "Epoch 25/30\n",
      "182421/182421 - 159s - loss: 3.6137 - accuracy: 0.3363 - 159s/epoch - 873us/step\n",
      "Epoch 26/30\n",
      "182421/182421 - 160s - loss: 3.6125 - accuracy: 0.3362 - 160s/epoch - 875us/step\n",
      "Epoch 27/30\n",
      "182421/182421 - 160s - loss: 3.6183 - accuracy: 0.3363 - 160s/epoch - 875us/step\n",
      "Epoch 28/30\n",
      "182421/182421 - 160s - loss: 3.6176 - accuracy: 0.3363 - 160s/epoch - 877us/step\n",
      "Epoch 29/30\n",
      "182421/182421 - 159s - loss: 3.6155 - accuracy: 0.3362 - 159s/epoch - 874us/step\n",
      "Epoch 30/30\n",
      "182421/182421 - 159s - loss: 3.6185 - accuracy: 0.3363 - 159s/epoch - 872us/step\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 4.192011833190918 - Accuracy: 42.91037619113922%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 4.386947154998779 - Accuracy: 34.08112823963165%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 4.110714435577393 - Accuracy: 32.651546597480774%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.142411231994629 - Accuracy: 30.370908975601196%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 3.8925857543945312 - Accuracy: 31.707316637039185%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 3.151974678039551 - Accuracy: 33.70942771434784%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.075552463531494 - Accuracy: 32.76896774768829%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 3.0900325775146484 - Accuracy: 32.036468386650085%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.4051899909973145 - Accuracy: 32.43595361709595%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 3.171452283859253 - Accuracy: 31.059956550598145%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 33.373205065727234 (+- 3.350568871636765)\n",
      "> Loss: 3.6618872404098513\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "met1 = dataset[[col for col in dataset.columns if col in cols]]\n",
    "\n",
    "X = met1.drop(columns=[\"Count\"])\n",
    "y = met1[\"Count\"]\n",
    "\n",
    "X = np.reshape(X.values, (X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = GroupKFold(n_splits=10)\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "model_met1 = Sequential()\n",
    "model_met1.add(LSTM(4, input_shape=(1,met1.shape[1]-1)))\n",
    "model_met1.add(Dense(1))\n",
    "model_met1.compile(loss='mean_squared_error', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for train, test in kfold.split(X, y, dataset[\"StationEnd\"]):\n",
    "    model_met1.fit(X[train], y[train], epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model_met1.evaluate(X[test], y[test], verbose=0)\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
