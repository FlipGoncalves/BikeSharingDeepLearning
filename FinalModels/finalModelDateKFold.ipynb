{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 15:50:46.642513: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-31 15:50:46.754729: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-31 15:50:46.755815: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-31 15:50:47.738507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../DateDatasets/dataset.csv\").drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"StationEnd\", \"WorkingDay\", \"Hour\", \"Count\", \"Count1\", \"Count1week\", \"Count2week\", \"Count3week\", \"Temp\", \"ATemp\", \"WeatherSituation\", \"Humidity\", \"Windspeed\"]\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 1\n",
    "verbose = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 16:50:29.343114: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-31 16:50:29.343657: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 16:50:29.650725: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-31 16:50:29.652202: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-31 16:50:29.652856: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-31 16:50:29.912219: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-31 16:50:29.913355: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-31 16:50:29.914029: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-31 16:50:30.243498: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-31 16:50:30.244518: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-31 16:50:30.245332: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183178/183178 - 165s - loss: 3.9782 - accuracy: 0.3327 - 165s/epoch - 902us/step\n",
      "Epoch 2/30\n",
      "183178/183178 - 160s - loss: 3.7695 - accuracy: 0.3338 - 160s/epoch - 874us/step\n",
      "Epoch 3/30\n",
      "183178/183178 - 160s - loss: 3.7548 - accuracy: 0.3338 - 160s/epoch - 874us/step\n",
      "Epoch 4/30\n",
      "183178/183178 - 160s - loss: 3.7390 - accuracy: 0.3337 - 160s/epoch - 875us/step\n",
      "Epoch 5/30\n",
      "183178/183178 - 161s - loss: 3.7317 - accuracy: 0.3338 - 161s/epoch - 877us/step\n",
      "Epoch 6/30\n",
      "183178/183178 - 160s - loss: 3.7321 - accuracy: 0.3337 - 160s/epoch - 876us/step\n",
      "Epoch 7/30\n",
      "183178/183178 - 161s - loss: 3.7320 - accuracy: 0.3336 - 161s/epoch - 881us/step\n",
      "Epoch 8/30\n",
      "183178/183178 - 158s - loss: 3.7263 - accuracy: 0.3336 - 158s/epoch - 864us/step\n",
      "Epoch 9/30\n",
      "183178/183178 - 160s - loss: 3.7271 - accuracy: 0.3336 - 160s/epoch - 874us/step\n",
      "Epoch 10/30\n",
      "183178/183178 - 160s - loss: 3.7196 - accuracy: 0.3336 - 160s/epoch - 873us/step\n",
      "Epoch 11/30\n",
      "183178/183178 - 160s - loss: 3.7166 - accuracy: 0.3335 - 160s/epoch - 875us/step\n",
      "Epoch 12/30\n",
      "183178/183178 - 160s - loss: 3.7180 - accuracy: 0.3335 - 160s/epoch - 876us/step\n",
      "Epoch 13/30\n",
      "183178/183178 - 160s - loss: 3.7219 - accuracy: 0.3336 - 160s/epoch - 875us/step\n",
      "Epoch 14/30\n",
      "183178/183178 - 160s - loss: 3.7165 - accuracy: 0.3336 - 160s/epoch - 875us/step\n",
      "Epoch 15/30\n",
      "183178/183178 - 160s - loss: 3.7155 - accuracy: 0.3336 - 160s/epoch - 874us/step\n",
      "Epoch 16/30\n",
      "183178/183178 - 160s - loss: 3.7129 - accuracy: 0.3335 - 160s/epoch - 874us/step\n",
      "Epoch 17/30\n",
      "183178/183178 - 160s - loss: 3.7108 - accuracy: 0.3336 - 160s/epoch - 872us/step\n",
      "Epoch 18/30\n",
      "183178/183178 - 160s - loss: 3.7078 - accuracy: 0.3335 - 160s/epoch - 874us/step\n",
      "Epoch 19/30\n",
      "183178/183178 - 160s - loss: 3.7111 - accuracy: 0.3335 - 160s/epoch - 873us/step\n",
      "Epoch 20/30\n",
      "183178/183178 - 160s - loss: 3.7132 - accuracy: 0.3334 - 160s/epoch - 875us/step\n",
      "Epoch 21/30\n",
      "183178/183178 - 160s - loss: 3.7061 - accuracy: 0.3335 - 160s/epoch - 873us/step\n",
      "Epoch 22/30\n",
      "183178/183178 - 160s - loss: 3.7051 - accuracy: 0.3334 - 160s/epoch - 874us/step\n",
      "Epoch 23/30\n",
      "183178/183178 - 160s - loss: 3.7061 - accuracy: 0.3336 - 160s/epoch - 875us/step\n",
      "Epoch 24/30\n",
      "183178/183178 - 160s - loss: 3.7021 - accuracy: 0.3335 - 160s/epoch - 875us/step\n",
      "Epoch 25/30\n",
      "183178/183178 - 160s - loss: 3.6985 - accuracy: 0.3335 - 160s/epoch - 872us/step\n",
      "Epoch 26/30\n",
      "183178/183178 - 160s - loss: 3.6966 - accuracy: 0.3335 - 160s/epoch - 871us/step\n",
      "Epoch 27/30\n",
      "183178/183178 - 160s - loss: 3.6954 - accuracy: 0.3336 - 160s/epoch - 872us/step\n",
      "Epoch 28/30\n",
      "183178/183178 - 160s - loss: 3.6968 - accuracy: 0.3336 - 160s/epoch - 873us/step\n",
      "Epoch 29/30\n",
      "183178/183178 - 160s - loss: 3.6934 - accuracy: 0.3336 - 160s/epoch - 872us/step\n",
      "Epoch 30/30\n",
      "183178/183178 - 161s - loss: 3.6916 - accuracy: 0.3335 - 161s/epoch - 878us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 18:10:37.968619: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-31 18:10:37.970081: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-31 18:10:37.970895: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "183010/183010 - 162s - loss: 3.6736 - accuracy: 0.3336 - 162s/epoch - 885us/step\n",
      "Epoch 2/30\n",
      "183010/183010 - 162s - loss: 3.6738 - accuracy: 0.3336 - 162s/epoch - 887us/step\n",
      "Epoch 3/30\n",
      "183010/183010 - 162s - loss: 3.6693 - accuracy: 0.3336 - 162s/epoch - 885us/step\n",
      "Epoch 4/30\n",
      "183010/183010 - 162s - loss: 3.6668 - accuracy: 0.3336 - 162s/epoch - 883us/step\n",
      "Epoch 5/30\n",
      "183010/183010 - 162s - loss: 3.6732 - accuracy: 0.3336 - 162s/epoch - 883us/step\n",
      "Epoch 6/30\n",
      "183010/183010 - 162s - loss: 3.6665 - accuracy: 0.3335 - 162s/epoch - 884us/step\n",
      "Epoch 7/30\n",
      "183010/183010 - 161s - loss: 3.6646 - accuracy: 0.3336 - 161s/epoch - 881us/step\n",
      "Epoch 8/30\n",
      "183010/183010 - 161s - loss: 3.6607 - accuracy: 0.3335 - 161s/epoch - 879us/step\n",
      "Epoch 9/30\n",
      "183010/183010 - 161s - loss: 3.6621 - accuracy: 0.3335 - 161s/epoch - 881us/step\n",
      "Epoch 10/30\n",
      "183010/183010 - 162s - loss: 3.6620 - accuracy: 0.3335 - 162s/epoch - 883us/step\n",
      "Epoch 11/30\n",
      "183010/183010 - 161s - loss: 3.6606 - accuracy: 0.3334 - 161s/epoch - 882us/step\n",
      "Epoch 12/30\n",
      "183010/183010 - 162s - loss: 3.6562 - accuracy: 0.3335 - 162s/epoch - 883us/step\n",
      "Epoch 13/30\n",
      "183010/183010 - 162s - loss: 3.6547 - accuracy: 0.3334 - 162s/epoch - 883us/step\n",
      "Epoch 14/30\n",
      "183010/183010 - 162s - loss: 3.6502 - accuracy: 0.3335 - 162s/epoch - 884us/step\n",
      "Epoch 15/30\n",
      "183010/183010 - 162s - loss: 3.6543 - accuracy: 0.3334 - 162s/epoch - 885us/step\n",
      "Epoch 16/30\n",
      "183010/183010 - 162s - loss: 3.6527 - accuracy: 0.3334 - 162s/epoch - 884us/step\n",
      "Epoch 17/30\n",
      "183010/183010 - 161s - loss: 3.6475 - accuracy: 0.3335 - 161s/epoch - 881us/step\n",
      "Epoch 18/30\n",
      "183010/183010 - 161s - loss: 3.6442 - accuracy: 0.3334 - 161s/epoch - 881us/step\n",
      "Epoch 19/30\n",
      "183010/183010 - 161s - loss: 3.6478 - accuracy: 0.3334 - 161s/epoch - 882us/step\n",
      "Epoch 20/30\n",
      "183010/183010 - 161s - loss: 3.6483 - accuracy: 0.3334 - 161s/epoch - 881us/step\n",
      "Epoch 21/30\n",
      "183010/183010 - 161s - loss: 3.6411 - accuracy: 0.3333 - 161s/epoch - 882us/step\n",
      "Epoch 22/30\n",
      "183010/183010 - 161s - loss: 3.6399 - accuracy: 0.3333 - 161s/epoch - 882us/step\n",
      "Epoch 23/30\n",
      "183010/183010 - 161s - loss: 3.6409 - accuracy: 0.3334 - 161s/epoch - 879us/step\n",
      "Epoch 24/30\n",
      "183010/183010 - 161s - loss: 3.6363 - accuracy: 0.3334 - 161s/epoch - 879us/step\n",
      "Epoch 25/30\n",
      "183010/183010 - 161s - loss: 3.6482 - accuracy: 0.3334 - 161s/epoch - 879us/step\n",
      "Epoch 26/30\n",
      "183010/183010 - 161s - loss: 3.6443 - accuracy: 0.3333 - 161s/epoch - 880us/step\n",
      "Epoch 27/30\n",
      "183010/183010 - 161s - loss: 3.6396 - accuracy: 0.3333 - 161s/epoch - 881us/step\n",
      "Epoch 28/30\n",
      "183010/183010 - 161s - loss: 3.6401 - accuracy: 0.3333 - 161s/epoch - 882us/step\n",
      "Epoch 29/30\n",
      "183010/183010 - 162s - loss: 3.6448 - accuracy: 0.3334 - 162s/epoch - 883us/step\n",
      "Epoch 30/30\n",
      "183010/183010 - 161s - loss: 3.6398 - accuracy: 0.3334 - 161s/epoch - 881us/step\n",
      "Epoch 1/30\n",
      "182982/182982 - 161s - loss: 3.6567 - accuracy: 0.3332 - 161s/epoch - 881us/step\n",
      "Epoch 2/30\n",
      "182982/182982 - 161s - loss: 3.6548 - accuracy: 0.3332 - 161s/epoch - 881us/step\n",
      "Epoch 3/30\n",
      "182982/182982 - 161s - loss: 3.6568 - accuracy: 0.3332 - 161s/epoch - 880us/step\n",
      "Epoch 4/30\n",
      "182982/182982 - 161s - loss: 3.6534 - accuracy: 0.3332 - 161s/epoch - 882us/step\n",
      "Epoch 5/30\n",
      "182982/182982 - 162s - loss: 3.6549 - accuracy: 0.3332 - 162s/epoch - 883us/step\n",
      "Epoch 6/30\n",
      "182982/182982 - 161s - loss: 3.6587 - accuracy: 0.3332 - 161s/epoch - 882us/step\n",
      "Epoch 7/30\n",
      "182982/182982 - 162s - loss: 3.6545 - accuracy: 0.3333 - 162s/epoch - 883us/step\n",
      "Epoch 8/30\n",
      "182982/182982 - 161s - loss: 3.6536 - accuracy: 0.3331 - 161s/epoch - 880us/step\n",
      "Epoch 9/30\n",
      "182982/182982 - 161s - loss: 3.6517 - accuracy: 0.3331 - 161s/epoch - 879us/step\n",
      "Epoch 10/30\n",
      "182982/182982 - 161s - loss: 3.6526 - accuracy: 0.3332 - 161s/epoch - 880us/step\n",
      "Epoch 11/30\n",
      "182982/182982 - 161s - loss: 3.6549 - accuracy: 0.3332 - 161s/epoch - 881us/step\n",
      "Epoch 12/30\n",
      "182982/182982 - 161s - loss: 3.6543 - accuracy: 0.3332 - 161s/epoch - 881us/step\n",
      "Epoch 13/30\n",
      "182982/182982 - 161s - loss: 3.6516 - accuracy: 0.3332 - 161s/epoch - 880us/step\n",
      "Epoch 14/30\n",
      "182982/182982 - 161s - loss: 3.6507 - accuracy: 0.3332 - 161s/epoch - 881us/step\n",
      "Epoch 15/30\n",
      "182982/182982 - 161s - loss: 3.6500 - accuracy: 0.3333 - 161s/epoch - 881us/step\n",
      "Epoch 16/30\n",
      "182982/182982 - 161s - loss: 3.6494 - accuracy: 0.3333 - 161s/epoch - 881us/step\n",
      "Epoch 17/30\n",
      "182982/182982 - 161s - loss: 3.6530 - accuracy: 0.3333 - 161s/epoch - 881us/step\n",
      "Epoch 18/30\n",
      "182982/182982 - 161s - loss: 3.6526 - accuracy: 0.3333 - 161s/epoch - 882us/step\n",
      "Epoch 19/30\n",
      "182982/182982 - 161s - loss: 3.6526 - accuracy: 0.3332 - 161s/epoch - 879us/step\n",
      "Epoch 20/30\n",
      "182982/182982 - 161s - loss: 3.6467 - accuracy: 0.3331 - 161s/epoch - 880us/step\n",
      "Epoch 21/30\n",
      "182982/182982 - 161s - loss: 3.6497 - accuracy: 0.3332 - 161s/epoch - 880us/step\n",
      "Epoch 22/30\n",
      "182982/182982 - 161s - loss: 3.6453 - accuracy: 0.3332 - 161s/epoch - 879us/step\n",
      "Epoch 23/30\n",
      "182982/182982 - 161s - loss: 3.6506 - accuracy: 0.3332 - 161s/epoch - 879us/step\n",
      "Epoch 24/30\n",
      "182982/182982 - 161s - loss: 3.6502 - accuracy: 0.3332 - 161s/epoch - 877us/step\n",
      "Epoch 25/30\n",
      "182982/182982 - 161s - loss: 3.6471 - accuracy: 0.3331 - 161s/epoch - 879us/step\n",
      "Epoch 26/30\n",
      "182982/182982 - 161s - loss: 3.6463 - accuracy: 0.3332 - 161s/epoch - 878us/step\n",
      "Epoch 27/30\n",
      "182982/182982 - 161s - loss: 3.6547 - accuracy: 0.3333 - 161s/epoch - 879us/step\n",
      "Epoch 28/30\n",
      "182982/182982 - 161s - loss: 3.6487 - accuracy: 0.3332 - 161s/epoch - 880us/step\n",
      "Epoch 29/30\n",
      "182982/182982 - 161s - loss: 3.6470 - accuracy: 0.3331 - 161s/epoch - 878us/step\n",
      "Epoch 30/30\n",
      "182982/182982 - 161s - loss: 3.6482 - accuracy: 0.3332 - 161s/epoch - 881us/step\n",
      "Epoch 1/30\n",
      "182971/182971 - 161s - loss: 3.6408 - accuracy: 0.3333 - 161s/epoch - 879us/step\n",
      "Epoch 2/30\n",
      "182971/182971 - 161s - loss: 3.6479 - accuracy: 0.3333 - 161s/epoch - 880us/step\n",
      "Epoch 3/30\n",
      "182971/182971 - 161s - loss: 3.6448 - accuracy: 0.3333 - 161s/epoch - 881us/step\n",
      "Epoch 4/30\n",
      "182971/182971 - 161s - loss: 3.6490 - accuracy: 0.3334 - 161s/epoch - 880us/step\n",
      "Epoch 5/30\n",
      "182971/182971 - 161s - loss: 3.6472 - accuracy: 0.3334 - 161s/epoch - 880us/step\n",
      "Epoch 6/30\n",
      "182971/182971 - 161s - loss: 3.6425 - accuracy: 0.3334 - 161s/epoch - 879us/step\n",
      "Epoch 7/30\n",
      "182971/182971 - 161s - loss: 3.6447 - accuracy: 0.3333 - 161s/epoch - 880us/step\n",
      "Epoch 8/30\n",
      "182971/182971 - 161s - loss: 3.6426 - accuracy: 0.3334 - 161s/epoch - 879us/step\n",
      "Epoch 9/30\n",
      "182971/182971 - 161s - loss: 3.6484 - accuracy: 0.3333 - 161s/epoch - 879us/step\n",
      "Epoch 10/30\n",
      "182971/182971 - 161s - loss: 3.6438 - accuracy: 0.3333 - 161s/epoch - 879us/step\n",
      "Epoch 11/30\n",
      "182971/182971 - 161s - loss: 3.6484 - accuracy: 0.3333 - 161s/epoch - 879us/step\n",
      "Epoch 12/30\n",
      "182971/182971 - 161s - loss: 3.6472 - accuracy: 0.3333 - 161s/epoch - 881us/step\n",
      "Epoch 13/30\n",
      "182971/182971 - 161s - loss: 3.6448 - accuracy: 0.3333 - 161s/epoch - 881us/step\n",
      "Epoch 14/30\n",
      "182971/182971 - 161s - loss: 3.6454 - accuracy: 0.3333 - 161s/epoch - 878us/step\n",
      "Epoch 15/30\n",
      "182971/182971 - 161s - loss: 3.6439 - accuracy: 0.3334 - 161s/epoch - 879us/step\n",
      "Epoch 16/30\n",
      "182971/182971 - 161s - loss: 3.6439 - accuracy: 0.3332 - 161s/epoch - 877us/step\n",
      "Epoch 17/30\n",
      "182971/182971 - 161s - loss: 3.6471 - accuracy: 0.3333 - 161s/epoch - 879us/step\n",
      "Epoch 18/30\n",
      "182971/182971 - 161s - loss: 3.6475 - accuracy: 0.3333 - 161s/epoch - 880us/step\n",
      "Epoch 19/30\n",
      "182971/182971 - 161s - loss: 3.6474 - accuracy: 0.3334 - 161s/epoch - 880us/step\n",
      "Epoch 20/30\n",
      "182971/182971 - 161s - loss: 3.6465 - accuracy: 0.3333 - 161s/epoch - 881us/step\n",
      "Epoch 21/30\n",
      "182971/182971 - 161s - loss: 3.6459 - accuracy: 0.3333 - 161s/epoch - 881us/step\n",
      "Epoch 22/30\n",
      "182971/182971 - 161s - loss: 3.6465 - accuracy: 0.3333 - 161s/epoch - 882us/step\n",
      "Epoch 23/30\n",
      "182971/182971 - 161s - loss: 3.6428 - accuracy: 0.3332 - 161s/epoch - 880us/step\n",
      "Epoch 24/30\n",
      "182971/182971 - 161s - loss: 3.6423 - accuracy: 0.3334 - 161s/epoch - 881us/step\n",
      "Epoch 25/30\n",
      "182971/182971 - 161s - loss: 3.6422 - accuracy: 0.3333 - 161s/epoch - 879us/step\n",
      "Epoch 26/30\n",
      "182971/182971 - 161s - loss: 3.6415 - accuracy: 0.3333 - 161s/epoch - 878us/step\n",
      "Epoch 27/30\n",
      "182971/182971 - 161s - loss: 3.6456 - accuracy: 0.3333 - 161s/epoch - 878us/step\n",
      "Epoch 28/30\n",
      "182971/182971 - 161s - loss: 3.6389 - accuracy: 0.3333 - 161s/epoch - 879us/step\n",
      "Epoch 29/30\n",
      "182971/182971 - 161s - loss: 3.6462 - accuracy: 0.3333 - 161s/epoch - 879us/step\n",
      "Epoch 30/30\n",
      "182971/182971 - 161s - loss: 3.6439 - accuracy: 0.3333 - 161s/epoch - 879us/step\n",
      "Epoch 1/30\n",
      "182971/182971 - 161s - loss: 3.6415 - accuracy: 0.3338 - 161s/epoch - 879us/step\n",
      "Epoch 2/30\n",
      "182971/182971 - 161s - loss: 3.6391 - accuracy: 0.3338 - 161s/epoch - 880us/step\n",
      "Epoch 3/30\n",
      "182971/182971 - 161s - loss: 3.6462 - accuracy: 0.3339 - 161s/epoch - 880us/step\n",
      "Epoch 4/30\n",
      "182971/182971 - 161s - loss: 3.6451 - accuracy: 0.3339 - 161s/epoch - 879us/step\n",
      "Epoch 5/30\n",
      "182971/182971 - 161s - loss: 3.6405 - accuracy: 0.3337 - 161s/epoch - 878us/step\n",
      "Epoch 6/30\n",
      "182971/182971 - 161s - loss: 3.6368 - accuracy: 0.3338 - 161s/epoch - 879us/step\n",
      "Epoch 7/30\n",
      "182971/182971 - 161s - loss: 3.6428 - accuracy: 0.3338 - 161s/epoch - 878us/step\n",
      "Epoch 8/30\n",
      "182971/182971 - 161s - loss: 3.6412 - accuracy: 0.3336 - 161s/epoch - 878us/step\n",
      "Epoch 9/30\n",
      "182971/182971 - 161s - loss: 3.6440 - accuracy: 0.3337 - 161s/epoch - 880us/step\n",
      "Epoch 10/30\n",
      "182971/182971 - 161s - loss: 3.6396 - accuracy: 0.3337 - 161s/epoch - 879us/step\n",
      "Epoch 11/30\n",
      "182971/182971 - 161s - loss: 3.6395 - accuracy: 0.3337 - 161s/epoch - 879us/step\n",
      "Epoch 12/30\n",
      "182971/182971 - 161s - loss: 3.6386 - accuracy: 0.3338 - 161s/epoch - 880us/step\n",
      "Epoch 13/30\n",
      "182971/182971 - 161s - loss: 3.6381 - accuracy: 0.3337 - 161s/epoch - 878us/step\n",
      "Epoch 14/30\n",
      "182971/182971 - 160s - loss: 3.6372 - accuracy: 0.3338 - 160s/epoch - 876us/step\n",
      "Epoch 15/30\n",
      "182971/182971 - 161s - loss: 3.6413 - accuracy: 0.3338 - 161s/epoch - 878us/step\n",
      "Epoch 16/30\n",
      "182971/182971 - 161s - loss: 3.6385 - accuracy: 0.3339 - 161s/epoch - 878us/step\n",
      "Epoch 17/30\n",
      "182971/182971 - 160s - loss: 3.6372 - accuracy: 0.3337 - 160s/epoch - 875us/step\n",
      "Epoch 18/30\n",
      "182971/182971 - 161s - loss: 3.6396 - accuracy: 0.3337 - 161s/epoch - 878us/step\n",
      "Epoch 19/30\n",
      "182971/182971 - 160s - loss: 3.6407 - accuracy: 0.3336 - 160s/epoch - 876us/step\n",
      "Epoch 20/30\n",
      "182971/182971 - 160s - loss: 3.6325 - accuracy: 0.3337 - 160s/epoch - 877us/step\n",
      "Epoch 21/30\n",
      "182971/182971 - 160s - loss: 3.6380 - accuracy: 0.3337 - 160s/epoch - 877us/step\n",
      "Epoch 22/30\n",
      "182971/182971 - 161s - loss: 3.6397 - accuracy: 0.3338 - 161s/epoch - 879us/step\n",
      "Epoch 23/30\n",
      "182971/182971 - 160s - loss: 3.6379 - accuracy: 0.3337 - 160s/epoch - 875us/step\n",
      "Epoch 24/30\n",
      "182971/182971 - 160s - loss: 3.6400 - accuracy: 0.3338 - 160s/epoch - 876us/step\n",
      "Epoch 25/30\n",
      "182971/182971 - 161s - loss: 3.6406 - accuracy: 0.3338 - 161s/epoch - 878us/step\n",
      "Epoch 26/30\n",
      "182971/182971 - 161s - loss: 3.6362 - accuracy: 0.3338 - 161s/epoch - 878us/step\n",
      "Epoch 27/30\n",
      "182971/182971 - 161s - loss: 3.6395 - accuracy: 0.3337 - 161s/epoch - 878us/step\n",
      "Epoch 28/30\n",
      "182971/182971 - 161s - loss: 3.6401 - accuracy: 0.3337 - 161s/epoch - 878us/step\n",
      "Epoch 29/30\n",
      "182971/182971 - 161s - loss: 3.6361 - accuracy: 0.3337 - 161s/epoch - 878us/step\n",
      "Epoch 30/30\n",
      "182971/182971 - 161s - loss: 3.6382 - accuracy: 0.3337 - 161s/epoch - 878us/step\n",
      "Epoch 1/30\n",
      "182972/182972 - 160s - loss: 3.6472 - accuracy: 0.3328 - 160s/epoch - 877us/step\n",
      "Epoch 2/30\n",
      "182972/182972 - 161s - loss: 3.6501 - accuracy: 0.3328 - 161s/epoch - 880us/step\n",
      "Epoch 3/30\n",
      "182972/182972 - 161s - loss: 3.6507 - accuracy: 0.3328 - 161s/epoch - 879us/step\n",
      "Epoch 4/30\n",
      "182972/182972 - 161s - loss: 3.6500 - accuracy: 0.3328 - 161s/epoch - 879us/step\n",
      "Epoch 5/30\n",
      "182972/182972 - 161s - loss: 3.6520 - accuracy: 0.3328 - 161s/epoch - 878us/step\n",
      "Epoch 6/30\n",
      "182972/182972 - 161s - loss: 3.6497 - accuracy: 0.3329 - 161s/epoch - 879us/step\n",
      "Epoch 7/30\n",
      "182972/182972 - 161s - loss: 3.6500 - accuracy: 0.3328 - 161s/epoch - 882us/step\n",
      "Epoch 8/30\n",
      "182972/182972 - 161s - loss: 3.6517 - accuracy: 0.3329 - 161s/epoch - 882us/step\n",
      "Epoch 9/30\n",
      "182972/182972 - 161s - loss: 3.6499 - accuracy: 0.3328 - 161s/epoch - 880us/step\n",
      "Epoch 10/30\n",
      "182972/182972 - 161s - loss: 3.6534 - accuracy: 0.3328 - 161s/epoch - 881us/step\n",
      "Epoch 11/30\n",
      "182972/182972 - 161s - loss: 3.6497 - accuracy: 0.3328 - 161s/epoch - 879us/step\n",
      "Epoch 12/30\n",
      "182972/182972 - 161s - loss: 3.6505 - accuracy: 0.3327 - 161s/epoch - 881us/step\n",
      "Epoch 13/30\n",
      "182972/182972 - 161s - loss: 3.6466 - accuracy: 0.3328 - 161s/epoch - 880us/step\n",
      "Epoch 14/30\n",
      "182972/182972 - 161s - loss: 3.6516 - accuracy: 0.3327 - 161s/epoch - 882us/step\n",
      "Epoch 15/30\n",
      "182972/182972 - 161s - loss: 3.6503 - accuracy: 0.3328 - 161s/epoch - 880us/step\n",
      "Epoch 16/30\n",
      "182972/182972 - 161s - loss: 3.6500 - accuracy: 0.3328 - 161s/epoch - 878us/step\n",
      "Epoch 17/30\n",
      "182972/182972 - 161s - loss: 3.6471 - accuracy: 0.3328 - 161s/epoch - 880us/step\n",
      "Epoch 18/30\n",
      "182972/182972 - 161s - loss: 3.6505 - accuracy: 0.3328 - 161s/epoch - 881us/step\n",
      "Epoch 19/30\n",
      "182972/182972 - 161s - loss: 3.6464 - accuracy: 0.3327 - 161s/epoch - 882us/step\n",
      "Epoch 20/30\n",
      "182972/182972 - 161s - loss: 3.6538 - accuracy: 0.3327 - 161s/epoch - 881us/step\n",
      "Epoch 21/30\n",
      "182972/182972 - 162s - loss: 3.6520 - accuracy: 0.3328 - 162s/epoch - 883us/step\n",
      "Epoch 22/30\n",
      "182972/182972 - 161s - loss: 3.6509 - accuracy: 0.3328 - 161s/epoch - 880us/step\n",
      "Epoch 23/30\n",
      "182972/182972 - 160s - loss: 3.6535 - accuracy: 0.3328 - 160s/epoch - 877us/step\n",
      "Epoch 24/30\n",
      "182972/182972 - 161s - loss: 3.6531 - accuracy: 0.3328 - 161s/epoch - 882us/step\n",
      "Epoch 25/30\n",
      "182972/182972 - 161s - loss: 3.6514 - accuracy: 0.3327 - 161s/epoch - 880us/step\n",
      "Epoch 26/30\n",
      "182972/182972 - 161s - loss: 3.6478 - accuracy: 0.3327 - 161s/epoch - 879us/step\n",
      "Epoch 27/30\n",
      "182972/182972 - 161s - loss: 3.6515 - accuracy: 0.3327 - 161s/epoch - 879us/step\n",
      "Epoch 28/30\n",
      "182972/182972 - 161s - loss: 3.6478 - accuracy: 0.3328 - 161s/epoch - 880us/step\n",
      "Epoch 29/30\n",
      "182972/182972 - 161s - loss: 3.6525 - accuracy: 0.3328 - 161s/epoch - 880us/step\n",
      "Epoch 30/30\n",
      "182972/182972 - 161s - loss: 3.6544 - accuracy: 0.3328 - 161s/epoch - 882us/step\n",
      "Epoch 1/30\n",
      "183007/183007 - 163s - loss: 3.6478 - accuracy: 0.3327 - 163s/epoch - 890us/step\n",
      "Epoch 2/30\n",
      "183007/183007 - 163s - loss: 3.6431 - accuracy: 0.3326 - 163s/epoch - 889us/step\n",
      "Epoch 3/30\n",
      "183007/183007 - 162s - loss: 3.6497 - accuracy: 0.3326 - 162s/epoch - 886us/step\n",
      "Epoch 4/30\n",
      "183007/183007 - 162s - loss: 3.6452 - accuracy: 0.3327 - 162s/epoch - 884us/step\n",
      "Epoch 5/30\n",
      "183007/183007 - 162s - loss: 3.6497 - accuracy: 0.3326 - 162s/epoch - 888us/step\n",
      "Epoch 6/30\n",
      "183007/183007 - 163s - loss: 3.6502 - accuracy: 0.3328 - 163s/epoch - 890us/step\n",
      "Epoch 7/30\n",
      "183007/183007 - 163s - loss: 3.6475 - accuracy: 0.3328 - 163s/epoch - 889us/step\n",
      "Epoch 8/30\n",
      "183007/183007 - 163s - loss: 3.6453 - accuracy: 0.3327 - 163s/epoch - 889us/step\n",
      "Epoch 9/30\n",
      "183007/183007 - 163s - loss: 3.6492 - accuracy: 0.3328 - 163s/epoch - 890us/step\n",
      "Epoch 10/30\n",
      "183007/183007 - 163s - loss: 3.6464 - accuracy: 0.3328 - 163s/epoch - 889us/step\n",
      "Epoch 11/30\n",
      "183007/183007 - 163s - loss: 3.6461 - accuracy: 0.3328 - 163s/epoch - 889us/step\n",
      "Epoch 12/30\n",
      "183007/183007 - 163s - loss: 3.6411 - accuracy: 0.3327 - 163s/epoch - 889us/step\n",
      "Epoch 13/30\n",
      "183007/183007 - 162s - loss: 3.6444 - accuracy: 0.3327 - 162s/epoch - 887us/step\n",
      "Epoch 14/30\n",
      "183007/183007 - 163s - loss: 3.6515 - accuracy: 0.3328 - 163s/epoch - 888us/step\n",
      "Epoch 15/30\n",
      "183007/183007 - 163s - loss: 3.6452 - accuracy: 0.3327 - 163s/epoch - 888us/step\n",
      "Epoch 16/30\n",
      "183007/183007 - 163s - loss: 3.6441 - accuracy: 0.3327 - 163s/epoch - 890us/step\n",
      "Epoch 17/30\n",
      "183007/183007 - 162s - loss: 3.6501 - accuracy: 0.3328 - 162s/epoch - 888us/step\n",
      "Epoch 18/30\n",
      "183007/183007 - 162s - loss: 3.6513 - accuracy: 0.3327 - 162s/epoch - 888us/step\n",
      "Epoch 19/30\n",
      "183007/183007 - 162s - loss: 3.6485 - accuracy: 0.3327 - 162s/epoch - 887us/step\n",
      "Epoch 20/30\n",
      "183007/183007 - 163s - loss: 3.6463 - accuracy: 0.3327 - 163s/epoch - 888us/step\n",
      "Epoch 21/30\n",
      "183007/183007 - 162s - loss: 3.6440 - accuracy: 0.3327 - 162s/epoch - 886us/step\n",
      "Epoch 22/30\n",
      "183007/183007 - 162s - loss: 3.6446 - accuracy: 0.3328 - 162s/epoch - 885us/step\n",
      "Epoch 23/30\n",
      "183007/183007 - 162s - loss: 3.6448 - accuracy: 0.3327 - 162s/epoch - 884us/step\n",
      "Epoch 24/30\n",
      "183007/183007 - 162s - loss: 3.6472 - accuracy: 0.3327 - 162s/epoch - 886us/step\n",
      "Epoch 25/30\n",
      "183007/183007 - 163s - loss: 3.6504 - accuracy: 0.3327 - 163s/epoch - 889us/step\n",
      "Epoch 26/30\n",
      "183007/183007 - 163s - loss: 3.6468 - accuracy: 0.3326 - 163s/epoch - 889us/step\n",
      "Epoch 27/30\n",
      "183007/183007 - 163s - loss: 3.6491 - accuracy: 0.3328 - 163s/epoch - 890us/step\n",
      "Epoch 28/30\n",
      "183007/183007 - 162s - loss: 3.6475 - accuracy: 0.3326 - 162s/epoch - 886us/step\n",
      "Epoch 29/30\n",
      "183007/183007 - 163s - loss: 3.6411 - accuracy: 0.3327 - 163s/epoch - 889us/step\n",
      "Epoch 30/30\n",
      "183007/183007 - 163s - loss: 3.6466 - accuracy: 0.3327 - 163s/epoch - 890us/step\n",
      "Epoch 1/30\n",
      "183180/183180 - 161s - loss: 3.6369 - accuracy: 0.3334 - 161s/epoch - 879us/step\n",
      "Epoch 2/30\n",
      "183180/183180 - 161s - loss: 3.6374 - accuracy: 0.3335 - 161s/epoch - 880us/step\n",
      "Epoch 3/30\n",
      "183180/183180 - 162s - loss: 3.6363 - accuracy: 0.3335 - 162s/epoch - 883us/step\n",
      "Epoch 4/30\n",
      "183180/183180 - 161s - loss: 3.6348 - accuracy: 0.3336 - 161s/epoch - 882us/step\n",
      "Epoch 5/30\n",
      "183180/183180 - 161s - loss: 3.6366 - accuracy: 0.3334 - 161s/epoch - 879us/step\n",
      "Epoch 6/30\n",
      "183180/183180 - 161s - loss: 3.6394 - accuracy: 0.3334 - 161s/epoch - 880us/step\n",
      "Epoch 7/30\n",
      "183180/183180 - 162s - loss: 3.6394 - accuracy: 0.3336 - 162s/epoch - 882us/step\n",
      "Epoch 8/30\n",
      "183180/183180 - 161s - loss: 3.6374 - accuracy: 0.3335 - 161s/epoch - 880us/step\n",
      "Epoch 9/30\n",
      "183180/183180 - 161s - loss: 3.6346 - accuracy: 0.3336 - 161s/epoch - 879us/step\n",
      "Epoch 10/30\n",
      "183180/183180 - 161s - loss: 3.6389 - accuracy: 0.3336 - 161s/epoch - 881us/step\n",
      "Epoch 11/30\n",
      "183180/183180 - 161s - loss: 3.6403 - accuracy: 0.3335 - 161s/epoch - 878us/step\n",
      "Epoch 12/30\n",
      "183180/183180 - 161s - loss: 3.6349 - accuracy: 0.3334 - 161s/epoch - 879us/step\n",
      "Epoch 13/30\n",
      "183180/183180 - 162s - loss: 3.6347 - accuracy: 0.3335 - 162s/epoch - 882us/step\n",
      "Epoch 14/30\n",
      "183180/183180 - 161s - loss: 3.6375 - accuracy: 0.3334 - 161s/epoch - 881us/step\n",
      "Epoch 15/30\n",
      "183180/183180 - 162s - loss: 3.6369 - accuracy: 0.3335 - 162s/epoch - 882us/step\n",
      "Epoch 16/30\n",
      "183180/183180 - 162s - loss: 3.6372 - accuracy: 0.3335 - 162s/epoch - 882us/step\n",
      "Epoch 17/30\n",
      "183180/183180 - 162s - loss: 3.6400 - accuracy: 0.3335 - 162s/epoch - 882us/step\n",
      "Epoch 18/30\n",
      "183180/183180 - 161s - loss: 3.6391 - accuracy: 0.3334 - 161s/epoch - 881us/step\n",
      "Epoch 19/30\n",
      "183180/183180 - 162s - loss: 3.6340 - accuracy: 0.3334 - 162s/epoch - 882us/step\n",
      "Epoch 20/30\n",
      "183180/183180 - 161s - loss: 3.6381 - accuracy: 0.3334 - 161s/epoch - 879us/step\n",
      "Epoch 21/30\n",
      "183180/183180 - 162s - loss: 3.6461 - accuracy: 0.3335 - 162s/epoch - 882us/step\n",
      "Epoch 22/30\n",
      "183180/183180 - 161s - loss: 3.6440 - accuracy: 0.3335 - 161s/epoch - 880us/step\n",
      "Epoch 23/30\n",
      "183180/183180 - 161s - loss: 3.6413 - accuracy: 0.3335 - 161s/epoch - 880us/step\n",
      "Epoch 24/30\n",
      "183180/183180 - 161s - loss: 3.6364 - accuracy: 0.3334 - 161s/epoch - 880us/step\n",
      "Epoch 25/30\n",
      "183180/183180 - 161s - loss: 3.6412 - accuracy: 0.3335 - 161s/epoch - 881us/step\n",
      "Epoch 26/30\n",
      "183180/183180 - 161s - loss: 3.6393 - accuracy: 0.3335 - 161s/epoch - 879us/step\n",
      "Epoch 27/30\n",
      "183180/183180 - 161s - loss: 3.6363 - accuracy: 0.3335 - 161s/epoch - 879us/step\n",
      "Epoch 28/30\n",
      "183180/183180 - 161s - loss: 3.6397 - accuracy: 0.3335 - 161s/epoch - 878us/step\n",
      "Epoch 29/30\n",
      "183180/183180 - 161s - loss: 3.6378 - accuracy: 0.3336 - 161s/epoch - 881us/step\n",
      "Epoch 30/30\n",
      "183180/183180 - 161s - loss: 3.6355 - accuracy: 0.3335 - 161s/epoch - 879us/step\n",
      "Epoch 1/30\n",
      "183176/183176 - 161s - loss: 3.6045 - accuracy: 0.3332 - 161s/epoch - 879us/step\n",
      "Epoch 2/30\n",
      "183176/183176 - 161s - loss: 3.6120 - accuracy: 0.3332 - 161s/epoch - 880us/step\n",
      "Epoch 3/30\n",
      "183176/183176 - 161s - loss: 3.6113 - accuracy: 0.3332 - 161s/epoch - 878us/step\n",
      "Epoch 4/30\n",
      "183176/183176 - 161s - loss: 3.6112 - accuracy: 0.3332 - 161s/epoch - 878us/step\n",
      "Epoch 5/30\n",
      "183176/183176 - 161s - loss: 3.6127 - accuracy: 0.3331 - 161s/epoch - 880us/step\n",
      "Epoch 6/30\n",
      "183176/183176 - 162s - loss: 3.6156 - accuracy: 0.3332 - 162s/epoch - 882us/step\n",
      "Epoch 7/30\n",
      "183176/183176 - 161s - loss: 3.6095 - accuracy: 0.3332 - 161s/epoch - 879us/step\n",
      "Epoch 8/30\n",
      "183176/183176 - 161s - loss: 3.6138 - accuracy: 0.3330 - 161s/epoch - 881us/step\n",
      "Epoch 9/30\n",
      "183176/183176 - 162s - loss: 3.6097 - accuracy: 0.3331 - 162s/epoch - 882us/step\n",
      "Epoch 10/30\n",
      "183176/183176 - 161s - loss: 3.6123 - accuracy: 0.3331 - 161s/epoch - 881us/step\n",
      "Epoch 11/30\n",
      "183176/183176 - 161s - loss: 3.6106 - accuracy: 0.3332 - 161s/epoch - 879us/step\n",
      "Epoch 12/30\n",
      "183176/183176 - 161s - loss: 3.6107 - accuracy: 0.3332 - 161s/epoch - 881us/step\n",
      "Epoch 13/30\n",
      "183176/183176 - 161s - loss: 3.6126 - accuracy: 0.3331 - 161s/epoch - 880us/step\n",
      "Epoch 14/30\n",
      "183176/183176 - 161s - loss: 3.6118 - accuracy: 0.3331 - 161s/epoch - 879us/step\n",
      "Epoch 15/30\n",
      "183176/183176 - 161s - loss: 3.6134 - accuracy: 0.3331 - 161s/epoch - 881us/step\n",
      "Epoch 16/30\n",
      "183176/183176 - 162s - loss: 3.6138 - accuracy: 0.3331 - 162s/epoch - 882us/step\n",
      "Epoch 17/30\n",
      "183176/183176 - 161s - loss: 3.6129 - accuracy: 0.3332 - 161s/epoch - 879us/step\n",
      "Epoch 18/30\n",
      "183176/183176 - 161s - loss: 3.6091 - accuracy: 0.3331 - 161s/epoch - 879us/step\n",
      "Epoch 19/30\n",
      "183176/183176 - 161s - loss: 3.6125 - accuracy: 0.3333 - 161s/epoch - 882us/step\n",
      "Epoch 20/30\n",
      "183176/183176 - 161s - loss: 3.6100 - accuracy: 0.3332 - 161s/epoch - 881us/step\n",
      "Epoch 21/30\n",
      "183176/183176 - 162s - loss: 3.6130 - accuracy: 0.3332 - 162s/epoch - 883us/step\n",
      "Epoch 22/30\n",
      "183176/183176 - 161s - loss: 3.6133 - accuracy: 0.3331 - 161s/epoch - 881us/step\n",
      "Epoch 23/30\n",
      "183176/183176 - 161s - loss: 3.6111 - accuracy: 0.3331 - 161s/epoch - 881us/step\n",
      "Epoch 24/30\n",
      "183176/183176 - 161s - loss: 3.6139 - accuracy: 0.3331 - 161s/epoch - 881us/step\n",
      "Epoch 25/30\n",
      "183176/183176 - 162s - loss: 3.6101 - accuracy: 0.3331 - 162s/epoch - 882us/step\n",
      "Epoch 26/30\n",
      "183176/183176 - 161s - loss: 3.6101 - accuracy: 0.3332 - 161s/epoch - 880us/step\n",
      "Epoch 27/30\n",
      "183176/183176 - 161s - loss: 3.6115 - accuracy: 0.3331 - 161s/epoch - 882us/step\n",
      "Epoch 28/30\n",
      "183176/183176 - 161s - loss: 3.6142 - accuracy: 0.3331 - 161s/epoch - 879us/step\n",
      "Epoch 29/30\n",
      "183176/183176 - 161s - loss: 3.6051 - accuracy: 0.3330 - 161s/epoch - 880us/step\n",
      "Epoch 30/30\n",
      "183176/183176 - 161s - loss: 3.6122 - accuracy: 0.3332 - 161s/epoch - 879us/step\n",
      "Epoch 1/30\n",
      "183180/183180 - 161s - loss: 3.6339 - accuracy: 0.3335 - 161s/epoch - 880us/step\n",
      "Epoch 2/30\n",
      "183180/183180 - 161s - loss: 3.6379 - accuracy: 0.3335 - 161s/epoch - 881us/step\n",
      "Epoch 3/30\n",
      "183180/183180 - 162s - loss: 3.6387 - accuracy: 0.3335 - 162s/epoch - 883us/step\n",
      "Epoch 4/30\n",
      "183180/183180 - 161s - loss: 3.6357 - accuracy: 0.3335 - 161s/epoch - 881us/step\n",
      "Epoch 5/30\n",
      "183180/183180 - 162s - loss: 3.6395 - accuracy: 0.3334 - 162s/epoch - 882us/step\n",
      "Epoch 6/30\n",
      "183180/183180 - 162s - loss: 3.6381 - accuracy: 0.3335 - 162s/epoch - 882us/step\n",
      "Epoch 7/30\n",
      "183180/183180 - 162s - loss: 3.6355 - accuracy: 0.3334 - 162s/epoch - 882us/step\n",
      "Epoch 8/30\n",
      "183180/183180 - 161s - loss: 3.6366 - accuracy: 0.3335 - 161s/epoch - 880us/step\n",
      "Epoch 9/30\n",
      "183180/183180 - 162s - loss: 3.6397 - accuracy: 0.3336 - 162s/epoch - 884us/step\n",
      "Epoch 10/30\n",
      "183180/183180 - 162s - loss: 3.6351 - accuracy: 0.3335 - 162s/epoch - 884us/step\n",
      "Epoch 11/30\n",
      "183180/183180 - 162s - loss: 3.6375 - accuracy: 0.3335 - 162s/epoch - 885us/step\n",
      "Epoch 12/30\n",
      "183180/183180 - 162s - loss: 3.6350 - accuracy: 0.3336 - 162s/epoch - 883us/step\n",
      "Epoch 13/30\n",
      "183180/183180 - 162s - loss: 3.6374 - accuracy: 0.3335 - 162s/epoch - 883us/step\n",
      "Epoch 14/30\n",
      "183180/183180 - 162s - loss: 3.6393 - accuracy: 0.3335 - 162s/epoch - 883us/step\n",
      "Epoch 15/30\n",
      "183180/183180 - 162s - loss: 3.6363 - accuracy: 0.3334 - 162s/epoch - 884us/step\n",
      "Epoch 16/30\n",
      "183180/183180 - 162s - loss: 3.6341 - accuracy: 0.3336 - 162s/epoch - 885us/step\n",
      "Epoch 17/30\n",
      "183180/183180 - 162s - loss: 3.6364 - accuracy: 0.3336 - 162s/epoch - 884us/step\n",
      "Epoch 18/30\n",
      "183180/183180 - 162s - loss: 3.6432 - accuracy: 0.3336 - 162s/epoch - 884us/step\n",
      "Epoch 19/30\n",
      "183180/183180 - 162s - loss: 3.6384 - accuracy: 0.3334 - 162s/epoch - 882us/step\n",
      "Epoch 20/30\n",
      "183180/183180 - 162s - loss: 3.6372 - accuracy: 0.3335 - 162s/epoch - 884us/step\n",
      "Epoch 21/30\n",
      "183180/183180 - 162s - loss: 3.6414 - accuracy: 0.3335 - 162s/epoch - 886us/step\n",
      "Epoch 22/30\n",
      "183180/183180 - 162s - loss: 3.6408 - accuracy: 0.3335 - 162s/epoch - 884us/step\n",
      "Epoch 23/30\n",
      "183180/183180 - 162s - loss: 3.6399 - accuracy: 0.3335 - 162s/epoch - 884us/step\n",
      "Epoch 24/30\n",
      "183180/183180 - 162s - loss: 3.6424 - accuracy: 0.3335 - 162s/epoch - 886us/step\n",
      "Epoch 25/30\n",
      "183180/183180 - 162s - loss: 3.6371 - accuracy: 0.3335 - 162s/epoch - 886us/step\n",
      "Epoch 26/30\n",
      "183180/183180 - 162s - loss: 3.6382 - accuracy: 0.3335 - 162s/epoch - 883us/step\n",
      "Epoch 27/30\n",
      "183180/183180 - 160s - loss: 3.6363 - accuracy: 0.3335 - 160s/epoch - 873us/step\n",
      "Epoch 28/30\n",
      "183180/183180 - 155s - loss: 3.6380 - accuracy: 0.3335 - 155s/epoch - 847us/step\n",
      "Epoch 29/30\n",
      "183180/183180 - 155s - loss: 3.6358 - accuracy: 0.3335 - 155s/epoch - 848us/step\n",
      "Epoch 30/30\n",
      "183180/183180 - 151s - loss: 3.6383 - accuracy: 0.3335 - 151s/epoch - 825us/step\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 3.579608678817749 - Accuracy: 33.39925706386566%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 3.778212308883667 - Accuracy: 33.207473158836365%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 3.652766227722168 - Accuracy: 33.41168463230133%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 3.656916856765747 - Accuracy: 33.344754576683044%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 3.6568961143493652 - Accuracy: 32.943421602249146%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 3.535383939743042 - Accuracy: 33.7673157453537%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.5335206985473633 - Accuracy: 33.78603756427765%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 3.76507306098938 - Accuracy: 33.1553190946579%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.8430941104888916 - Accuracy: 33.386069536209106%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 3.6249265670776367 - Accuracy: 33.08609127998352%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 33.34874242544174 (+- 0.2578746814500845)\n",
      "> Loss: 3.662639856338501\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "met1 = dataset[[col for col in dataset.columns if col in cols]]\n",
    "\n",
    "X = met1.drop(columns=[\"Count\"])\n",
    "y = met1[\"Count\"]\n",
    "\n",
    "X = np.reshape(X.values, (X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = GroupKFold(n_splits=10)\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "model_met1 = Sequential()\n",
    "model_met1.add(LSTM(4, input_shape=(1,met1.shape[1]-1)))\n",
    "model_met1.add(Dense(1))\n",
    "model_met1.compile(loss='mean_squared_error', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for train, test in kfold.split(X, y, dataset[\"Day\"]):\n",
    "    model_met1.fit(X[train], y[train], epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model_met1.evaluate(X[test], y[test], verbose=0)\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
